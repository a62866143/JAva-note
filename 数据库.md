## 数据库

### 数据库基本概念

​		DataBase简称：DB

#### 什么是数据库

​	用于存储和管理数据的仓库

#### 数据库的特点

1. 持久化存储数据，本质就是一个文件系统
2. 方便存储和管理数据
3. 使用了统一的方式操作数据---------SQL

#### MySQL数据库软件

安装

卸载

配置



#### 配置

##### 	MySQL服务启动

1. 手动

2. cmd   services.msc  打开服务窗口

3. 使用管理员打开cmd

   net start mysql：启动mysql的服务

   net stop mysql： 关闭mysql服务

##### MYSQL登录

1. msqyl  -u  root  -p  密码

  2. msqyl  -hip  u root  -p  连接目标的密码
  3. msqyl  --host=ip --user=root --password=连接目标的密码  

##### MYSQL退出

   	1. exit
            	2. quit

##### MYSQL目录结构

1. MYSQL安装目录

   1. 配置文件 my.ini

2. MYSQL数据目录

   ​	/data

3. 概念

   数据库：文件

   表：文件

   数据：数据

### SQL

#### 什么是sql

Structured Query Language：结构化查询语言

其实就是定义了操作所以关系型数据库的规则，每一种数据库操作的方式存在不一样的地方，称为“方言”

#### SQL通用语法

1. 可以单行或者多行书写，并且以分号进行结尾

2. 可以使用空格和缩进来增加语句的可读性

3. MYSQL数据库的SQL语句不区分大小写，关键字建议大写

4. 3种注释方式

   单行注释：--注释内容或者 # 注释内容（mysql特有）

   多行注释： /*注释*/

### SQL分类

1、数据定义语言（[DDL](https://baike.baidu.com/item/DDL/21997)）：其语句包括动词CREATE,ALTER和DROP。在数据库中创建新表或修改、删除表（CREAT TABLE 或 DROP TABLE）；为表加入索引等。

2、[数据操作语言](https://baike.baidu.com/item/数据操作语言)（DML：Data Manipulation Language）：其语句包括动词[INSERT](https://baike.baidu.com/item/INSERT)、[UPDATE](https://baike.baidu.com/item/UPDATE)和[DELETE](https://baike.baidu.com/item/DELETE)。它们分别用于添加、修改和删除。

3、数据查询语言（[DQL](https://baike.baidu.com/item/DQL):Data Query Language）：其语句，也称为“数据检索[语句](https://baike.baidu.com/item/语句)”，用以从表中获得数据，确定数据怎样在应用程序给出。保留字[SELECT](https://baike.baidu.com/item/SELECT/10735068)是DQL（也是所有SQL）用得最多的动词，其他DQL常用的保留字有WHERE，ORDER BY，GROUP BY和HAVING。这些DQL保留字常与其它类型的SQL语句一起使用。

4、事务控制语言（TCL）：它的语句能确保被DML语句影响的表的所有行及时得以更新。包括COMMIT（提交）命令、SAVEPOINT（保存点）命令、ROLLBACK（回滚）命令。

5、[数据控制语言](https://baike.baidu.com/item/数据控制语言)（DCL）：它的语句通过GRANT或REVOKE实现权限控制，确定单个用户和用户组对[数据库对象](https://baike.baidu.com/item/数据库对象)的访问。某些RDBMS可用GRANT或REVOKE控制对[表单](https://baike.baidu.com/item/表单)个列的访问。

6、指针控制语言（CCL）：它的语句，像DECLARE CURSOR，FETCH INTO和UPDATE WHERE CURRENT用于对一个或多个表单独行的操作

#### **DDL**：操作数据库、表

1. ###### 操作数据库：CRUD

   1. C（Create）：创建

      1. 创建数据库：

         create database 数据库名称；

      2. 创建数据库，若不存在则创建

         create database if not exists 数据库名称；

      3. 创建数据库，并制定字符集

         create database 数据库名称 character set 字符集名 

   2. R（Rectrieve）：查询

      1. 查询所有数据库名称：

         ​	show databases；

      2. 查询某个数据库的字符集：查询某个数据库的创建语句

         show create database 数据库名称；

   3. U（Update）:修改（更新）

      1. 修改数据库的字符集

         alter database 数据库名称 character set 字符集名称；

   4. D（Delete）：删除

      1. 删除数据库

         drop database 数据库名称

      2. 判断数据库存在，存在则删除

         drop database if exists 数据库名称；

   5. 使用数据库

      1. 查询当前正在使用的数据库名称

         select database();

      2. 使用数据库

         use 数据库名称

2. ###### 操作表

   1. C（Create）创建：

      1. create table 表名（

         列名1 数据类型1，

         列名2 数据类型2，

         ...

         列名n 数据类型n，

         ）;

         **注意，最后一行不需要加逗号（，）**

         数据类型：

         1. int 整形

         2. double 小数类型

         3. data：日期，只包含年月日，yyyy-MM-dd

         4. datatime:日期，包含年月日时分秒 yyyy-MM-dd  HH：mm:ss

         5. timestamp: 时间戳类型 包含年月日时分秒 yyyy-MM-dd  HH：mm:ss

            如果将来不给这个字段赋值或赋值为null，则默认使用改单前的系统时间，来自动赋值

         6. varchar：字符串

         7. 

   2. R（Rectrieve）：查询

      1. 查询某个数据库中所以的表名称：

         ​	show tables；

      2. 查询表结构

         desc 表名；

   3. U（Update）:修改（更新）

      1. 修改表名

         alter table 表名 rename to 新的表名；

      2. 修改表的字符集

         alter table 表名 character set 字符集名称；

      3. 添加一列

         alter table 表名

      4. 修改列名称，类型

         alter table 表名 change 列名 新列别 新数据类型；

         alter table 表名 modify 列名 新数据类型；

      5. 删除列

         alter table 表名 drop 列名 ；

   4. D（Delete）：删除

      1. drop table
      2. drop table if exists 表名；

#### DML：增删改表中的数据

**1. 添加数据**

语法：

```
insert into 表名（列名1，列名2... 列名n） value （值1，值2 ...  值n）；
```

注意：

 1. 列名和值要一一对应

 2. 如果表名后，不顶用列名，则默认给所有列添加值

    insert into 表名 value （值1，值2 ...  值n）；

 3. 除了数字类型，其他类型需要使用引号（单双列都成）引起来

    

**2.删除数据**

语法：

​	delete from 表名 [where 条件]；

注意：

   	1. 如果不加条件，则会删除表中所有记录
            	2. 如果要删除所有记录
                     	1. delete from 表名；  不推荐使用。有多少条记录就会执行多少次删除操作
                     	2. TRUNCATE TABLE 表名； 推荐使用，效率更高，先删除表，然后再创建一张一样的表
         	3. 除了数字类型，其他类型需要使用引号（单双列都成）引起来

**3.修改数据**

语法：

​	update 表名 set 列名1=值1， 列名2=值2 ...  列名n=值n[where 条件]；

注意：

   	1. 如果不加任何条件，则会修改表中所有记录



#### DQL：查询语句

 1. 排序查询

    语法：order by 字句

    ```
    order by 排序字段1 排序方式1 ，排序字段2 排序方式2 ... 排序字段n 排序方式n
    ```

    排序方式：

    ​	ASC：升序，默认值

    ​	DESC：降序

    注意：

    ​	如果有多个排序条件，则当前面的条件值一致时才回判断后续条件

 2. 聚合查询

    1. count：计数

       1. 一般选择非空的列：主键

          1. count（*）

          2. max：最大值

             ```
             SELECT COUNT(*) FROM table_name
             SELECT COUNT(column_name) FROM table_name
             SELECT MAX(column_name) FROM table_name
             ```

       2. min：最小值

          ```
          SELECT MIN(column_name) FROM table_name
          ```

       3. sum：求和

          ```
          SELECT SUM(column_name) FROM table_name
          ```

       4. avg：平均值

       注意：聚合函数的计算，排除null值

       ​	解决方案：

       1. 选择不包含非空的列进行计算

         2. IFNULL函数

 3. 分组查询

          1. 语法：group by 分组字段；
          2. where和having的区别？
             1. where在分组之前进行限定，如果不满足条件，则不参与分组。having在分组之后进行限定，如果不满足结果，则不会被查询出来
             2. where后不可用跟聚合函数，having可以进行聚合函数的判断

 4. 分页查询

    1. 语法：limit开始的索引，每页查询的条数

    2. 公式：开始的索引=（当前页码-1）*每页显示的条数

       *每页显示三条记录

       ```
       SELECT * FROM student LIMIT 0，3；	第一页
       SELECT * FROM student LIMIT 3，3；	第二页
       SELECT * FROM student LIMIT 6，3；	第三页
       ```

       

    3. limit是一个“方言”



#### DQL总结

查询表中的记录



​	select * from 表名；

1. **语法**

   ```
   select
   ​		字段列表
   form
   ​		表名列表
   where
   ​		条件语句
   group by
   ​		分组字段
   having
   ​		分组之后的条件
   order by
   ​		排列
   limit
   ​		分页限定
   ```

   

2. **基础查询**

   1. 多个字段的查询

      sleect 字段名1，字段名2，...字段名n from 表名；

      如果查询所有字段，可以用 * 代替

   2. 去重查询

      distinct

   3. 计算

      一遍可以使用四则运算计算列一些列的参考值。（一般只会进行数据值的计算）

      if null（表达式1，表达式2...）：null参与的运算，计算结果都为null

      ​	表达式：哪个字段需要判断为null

      ​	如果该字段为null后的替代值

   4. 起别名

      as：as也可省略

   5. **条件查询**

      1. where字句后跟条件

      2. 运算符：

         between...and

         in(集合)

         like：模糊查询

         ​	占位符

         ​		_:单个任意字符

         ​		%:多个任意字符

         is null

         and 或 &&

         or 或 ||

         not 或 |

         

#### 例子

*查询年龄等于20岁

```
SELECT * FROM student WHERE age=20;
```



*查询年龄不等于20岁

```
SELECT * FROM student WHERE age！=20;

SELECT * FROM student WHERE age<>20;
```



*查询年龄在20到30岁

```
SELECT * FROM student WHERE age>=20 && age<=30;

SELECT * FROM student WHERE age>=20 AND age<=30;

SELECT * FROM student WHERE age BETWEEN 20 AND 30;
```



*查询年龄在20，25，30岁

```
SELECT * FROM student WHERE age=20  OR age=25 OR age=30;

SELECT * FROM student WHERE age IN（20，25，30）;
```



*查询英语成绩为NULL

```
SELECT * FROM student WHERE english IS NULL
```



*查询英语成绩为不NULL

```
SELECT * FROM student WHERE english IS NOT NULL
```



*查询姓马的同学有哪些？ like

```
SELECT * FROM student WHERE NAME LIKE  ‘马%’；
```



*查询第二个字是化的同学有哪些？ like

```
SELECT * FROM student WHERE NAME LIKE  ‘_化%’；
```



*查询第二个字是化名字为三个字的同学有哪些？ like

```
SELECT * FROM student WHERE NAME LIKE  ‘___’；
```



*查询名字中包含德的的同学有哪些？ like

```
SELECT * FROM student WHERE NAME LIKE  ‘%德%’；
```



DQL：查询语句

1. 排序查询
2. 聚合查询
3. 分组查询
4. 分页查询

约束

多表之间的关系

范式

数据库的备份和还原



### 约束

​	概念：对表中的数据进行限定，保证数据的正确性，有效性和完整性

​	分类：

 1. 主键约束：primary key

 2. 非空约束：not null

 3. 唯一约束：unique

 4. 外键约束：foreign key

    

    非空约束：not null

    1. 创建表时添加约束

       ```
        CREATE TABLE stu（
       ​	id INT，
        ​	NAME VARCHAR（20） NOT NULL
       ）；
       ```

5. 创建表完成后添加非空约束

       ALTER TABLE stu MODIFY NAME VARCHAR（20） NOT NULL ；

  6. 删除name的非空约束   

     ALTER TABLE stu MODIFY NAME VARCHAR（20）；

唯一约束
    

   1. 注意：
       唯一约束可以有NULL值，单是只能有一条记录为null

   2. 在创建表时，条件唯一约束

      ```
      CREATE TABLE stu（
      Id INT，
      phone_number VARCHAR(20) UNIQUE   ---手机号
      ）
      ```

3. 删除唯一约束

4. 再表创建完成后添加唯一约束

*主键约束：primary key

 1. 注意：

    1. 含义：非空且唯一
       2. 一张表只能有一个字段为主键
          3. 主键就是表中记录的唯一标识

    2. 在创建表时，添加主键约束

    ```
    create table stu(
    	id int primary key,
    	name varchar(20)
    );
    ```

    3. 删除主键

    ALTER TABLE stu DROP PRIMARY KEY;

    4. 创建表之后添加主键约束

    ALTER TABLE stu MODIFY id INT PRIMARRY KEY;

    5. 自动增长：

       1. 概念：如果某一列是数值类型的，使用auto_increment可以来完成值的自动增长
       2. 在创建表时，添加主键约束，并完成自动增长

```
create table stu(
	id int primary key auto_increment,
	name varchar(20)
);
```

   3. 删除自动增长

      ALTER TABLE stu MODIFY id INT；

   4. 添加自动增长

      ALTER TABLE stu MODIFY id INT AUTO_INCREMENT;

外键约束：foreign key，让表与表产生关系，从而保证数据的正确性

1. 在创建表时，可以添加外键

   语法：

   ​	

   ```
   	create table 表名（
   
   ​		...
   	外键列
   	constraint 外键名称 foreign key （外键列名称） references 主表名（主表列名）
   
   ）
   ```

   

2. 删除外键

   ALTER TABLE 表名 DROP FOREIGN KEY 外键表名；

3. 创建表后添加外键

   ALTER TABLE 表名 ADD CONSTRAINT 外键名称 FOREIGN KEY （外键字段名称） REFERENCES 主表名称（主表列名称）

4. 级联操作

   1. 添加级联操作

      ​	语法：ALTER TABLE 表名 ADD CONSTRAINT 外键名称

      ​			FOREIGN KEY （外键字段名称） REFEREMVES 主表名称 （主表列名）ON UPDATE CASCADE ON DELETE CASCADE；

   2. 分类

      1. 级联更新：ON UPDATE CASCADE
      2. 级联删除：ON UPDATE CASCADE

### 数据库的设计

1. 多表之间的关系

   1. 1对1
   2. 1对多
   3. 多对多

2. 实现关系

   1. 一对多

      实现方式：在多的一方建立外键指向一的一方

   2. 多对多

      实现方式：多对多关系实现续建筑第三张中间表，中间表至少包含两个字段，这两个字段作为第三章表的外键，分别指向两张表的主键

   3. 一对一

      实现方式：任意一方建立外键指向一的一方

#### 数据库设计的范式

概念：

​		设计数据库时，需要遵循一些规范。满足后面的范式的前提是必须先满足前面的范式。

​		设计数据库时，遵循不同的规范要求，设计出合理的关系型数据库，这些不同的规范被称为不同的范式，各种范式呈递次规范，越高的范式数据库冗余越小

​		目前关系熟即可有六种范式：

1. 第一范式：1NF
  2. 第二范式：2NF
  3. 第三范式：3NF
  4. BC范式：BCNF
  5. 第四范式：4NF
  6. 第五范式（完美范式）：5NF

分类：

 1. 第一范式（1NF）：每一列都是不可分割的原子数据项

 2. 第二范式（2NF）：在1NF的基础上，非码属性必须完全依赖于候选码（在1NF的基础上消除飞主属性对主码的部分函数依赖）

    使用的码必须全部都完全用得到，不要有可有可无的码

    y=f(x) 我现在有x和z同时也能推出y，但是z没有用到，此时y属于部分函数依赖

 3. 第三范式（3NF）：在2NF的基础上，任何非主属性不依赖于其他非主属性（在2NF基础上消除传递依赖）

    y=f(x)  z=f(y)    z此时就传递依赖于x，要直接依赖



### 多表查询 

查询语法：

​	select

​		列名列表

​	from

​		表名列表

​	where...



笛卡尔积：

​	有两个集合A，B，取这两个集合的所有组成情况。

​	要完成多表查询，需消除无用的数据

多表查询的分类：

 1. 内连接：

    1. 隐式内连接：使用where条件消除无用数据

       ```
       SELECT * FROM emp,dept WHERE emp.'dept_id'=dept.'id'
       ```

    2. 显式内连接

       语法：select	字段列表	from	表名1	inner	join	表名2	on	条件

       ```
       select * from emp inner	join dept on emp.'dept_id'=dept.'id'
       select * from emp join dept on emp.'dept_id'=dept.'id'
       ```

       inner可省略

 2. 外连接：

    1. 左外连接

       语法：

       ```
       select 字段列表 from 表1 left [outer] join 表2 on 条件；
       ```

       查询的是左表所有的数据及其交集部分

    2. 右外连接

       语法：

       ```
       select 字段列表 from 表1 right [outer] join 表2 on 条件；
       ```

       查询的是右表所有的数据及其交集部分

 3. 子查询：

    ​	概念：查询中嵌套查询，称嵌套查询为子查询

    子查询的不同情况

    1. 子查询的结果是单行单列的

       子查询可以作为条件，使用运算符去判断。

       查询员工工资小雨平均工资的人

       ```
       select * from emp where	emp.salary<(select avg(salary)from emp);
       ```

    2. 子查询的结果是多行单列的

       子查询可以作为条件，使用运算符in去判断。

       查询市场部和财务部所有员工的信息

       ```
       select id from emp where name='财务部' or name='市场部'
       select *  from emp where dept_id=3 or dept_id=2;
       ```

       子查询

       ```
       select *  from emp where dept_id in(select id from emp where name='财务部' or name='市场部');
       ```

       

    3. 子查询的结果是多行多列的

       子查询可以作为一张虚拟表参与查询

       查询员工入职日期是2011-11-11日之后的员工信息和部门信息

       子查询

       ```
       selecet * from dept ti, (select * from emp where emp.'join_date'>'2011-11-11') t2
       		where ti.id=t2.dept_id;
       ```

       普通内连接

       ```
       select * from emp t1，dept t2 where t1,'dept_id'=t2.'id' and t1.'join_date'>'2011-11-11'
       ```

       

### 事务

事务的基本介绍：

1. 概念：

   如果一个包含多个步骤的业务操作，被事务管理，那么这些操作要么同时成功，要么同时失败

2. 操作

   1. 开启事务： start transaction

   2. 回滚：rollback

   3. 提交：commit

      例子：银行转账

   4. mysql数据库中事务默认自动提交

      一条DML（增删改），事务自动提交一次

      事务自动提交的两种方式：

      ​	自动提交：

      ​			mysql就是自动提交

      ​	手动提交：

      ​			需先开启事务，再提交

      修改事务的默认提交方式：

      ​	查看事务的默认提交方式：SELECT @@autocmmit;	//自动提交

      ​	修改默认提交方式：set @@autocommit=0;	//手动提交

事务的四大特征：

#### 	ACID

1. 原子性(Atomicity)：是不可分割的最小操作单元，要么同时成功，要么同时失败
2. 持久性(Durability)：当事务提交或回滚后，数据库会持久化的保存数据
3. 隔离性(Isolation)：多个事务之间，相互独立
4. 一致性(Consistency)：事务操作前后，数据总量不变



按照严格的标准，只有同时满足ACID特性才是事务；但是在各大数据库厂商的实现中，真正满足ACID的事务少之又少。例如MySQL的NDB Cluster事务不满足持久性和隔离性；InnoDB默认事务隔离级别是可重复读，不满足隔离性；Oracle默认的事务隔离级别为READ COMMITTED，不满足隔离性……**因此与其说ACID是事务必须满足的条件，不如说它们是衡量事务的四个维度**。



#### ACID底层原理

简单来说，事务的 ACID 是通过 InnoDB 日志和锁来保证。

事务的**隔离性**是通过数据库锁的机制实现的，

与原子性、持久性侧重于研究事务本身不同，隔离性研究的是不同事务之间的相互影响。隔离性是指，事务内部的操作与其他事务是隔离的，并发执行的各个事务之间不能互相干扰。严格的隔离性，对应了事务隔离级别中的Serializable (可串行化)，但实际应用中出于性能方面的考虑很少会使用可串行化。

隔离性追求的是并发情形下事务之间互不干扰。简单起见，我们主要考虑最简单的读操作和写操作(加锁读等特殊读操作会特殊说明)，那么隔离性的探讨，主要可以分为两个方面：

- (一个事务)写操作对(另一个事务)写操作的影响：锁机制保证隔离性
- (一个事务)写操作对(另一个事务)读操作的影响：MVCC保证隔离性

##### 锁机制

首先来看两个事务的写操作之间的相互影响。隔离性要求同一时刻只能有一个事务对数据进行写操作，InnoDB通过锁机制来保证这一点。

锁机制的基本原理可以概括为：事务在修改数据之前，需要先获得相应的锁；获得锁之后，事务便可以修改数据；该事务操作期间，这部分数据是锁定的，其他事务如果需要修改数据，需要等待当前事务提交或回滚后释放锁。

**行锁与表锁**

按照粒度，锁可以分为表锁、行锁以及其他位于二者之间的锁。表锁在操作数据时会锁定整张表，并发性能较差；行锁则只锁定需要操作的数据，并发性能好。但是由于加锁本身需要消耗资源(获得锁、检查锁、释放锁等都需要消耗资源)，因此在锁定数据较多情况下使用表锁可以节省大量资源。MySQL中不同的存储引擎支持的锁是不一样的，InnoDB同时支持表锁和行锁，且出于性能考虑，绝大多数情况下使用的都是行锁。

**持久性**通过 Redo Log（重做日志）来实现，

InnoDB作为MySQL的存储引擎，数据是存放在磁盘中的，但如果每次读写数据都需要磁盘IO，效率会很低。为此，InnoDB提供了**缓存(Buffer Pool)**，Buffer Pool中包含了磁盘中部分数据页的映射，作为访问数据库的缓冲：当从数据库读取数据时，会首先从Buffer Pool中读取，如果Buffer Pool中没有，则从磁盘读取后放入Buffer Pool；当向数据库写入数据时，会首先写入Buffer Pool，**Buffer Pool中修改的数据会定期刷新到磁盘中**（这一过程称为**刷脏**）。

Buffer Pool的使用大大提高了读写数据的效率，但是也带了新的问题：如果MySQL宕机，而此时Buffer Pool中修改的数据还没有刷新到磁盘，就会导致数据的丢失，事务的持久性无法保证。

于是，redo log被引入来解决这个问题：当数据修改时，除了修改Buffer Pool中的数据，还会在**redo log记录这次操作**；当事务提交时，会调用fsync接口对redo log进行刷盘。如果MySQL宕机，重启时可以读取redo log中的数据，对数据库进行恢复。redo log采用的是WAL（Write-ahead logging，预写式日志），所有修改先写入日志，再更新到Buffer Pool，保证了数据不会因MySQL宕机而丢失，从而满足了持久性要求。

既然redo log也需要在事务提交时将日志写入磁盘，为什么它比直接将Buffer Pool中修改的数据写入磁盘(即刷脏)要快呢？主要有以下两方面的原因：

- 刷脏是随机IO，因为每次修改的数据位置随机，但写redo log是追加操作，属于顺序IO。
- 刷脏是以数据页（Page）为单位的，MySQL默认页大小是16KB，一个Page上一个小修改都要整页写入；而redo log中只包含真正需要写入的部分，无效IO大大减少。

**原子性**和**一致性**通过 Undo Log 来实现。

在说明原子性原理之前，首先介绍一下MySQL的事务日志。MySQL的日志有很多种，如二进制日志、错误日志、查询日志、慢查询日志等，此外InnoDB存储引擎还提供了两种事务日志：redo log(重做日志)和undo log(回滚日志)。其中redo log用于保证事务持久性；undo log则是事务原子性和隔离性实现的基础。

Undo Log的原理：
Undo Log 的原理很简单，为了满足事务的原子性，在操作任何数据之前，首先将数据**备份**到一个地方（这个存储数据备份的地方称为 Undo Log）。然后进行数据的修改。如果出现了错误或者用户执行了 Rollback 语句，系统可以利用 Undo Log 中的备份将数据恢复到事务开始之前的状态。

undo log属于**逻辑日志**，它记录的是sql执行相关的信息。当发生回滚时，**InnoDB会根据undo log的内容做与之前相反的工作**：对于每个insert，回滚时会执行delete；对于每个delete，回滚时会执行insert；对于每个update，回滚时会执行一个相反的update，把数据改回去。

Redo Log的原理：
和 Undo Log 相反，Redo Log 记录的是新数据的备份。在事务提交前，只要将 Redo Log 持久化即可，不需要将数据持久化。当系统崩溃时，虽然数据没有持久化，但是 Redo Log 已经持久化。系统可以根据 Redo Log 的内容，将所有数据恢复到最新的状态。



##### redo log与bin log

我们知道，在MySQL中还存在binlog(二进制日志)也可以记录写操作并用于数据的恢复，但二者是有着根本的不同的：

1. 作用不同：redo log是用于crash recovery的，保证MySQL宕机也不会影响持久性；binlog是用于point-in-time recovery的，保证服务器可以基于时间点恢复数据，此外binlog还用于主从复制。
2. 层次不同：redo log是InnoDB存储引擎实现的，而binlog是MySQL的服务器层(可以参考文章前面对MySQL逻辑架构的介绍)实现的，同时支持InnoDB和其他存储引擎。
3. 内容不同：redo log是物理日志，内容基于磁盘的Page；binlog的内容是二进制的，根据binlog_format参数的不同，可能基于sql语句、基于数据本身或者二者的混合。
4. 写入时机不同：binlog在事务提交时写入；redo log的写入时机相对多元：
   - 前面曾提到：当事务提交时会调用fsync对redo log进行刷盘；这是默认情况下的策略，修改innodb_flush_log_at_trx_commit参数可以改变该策略，但事务的持久性将无法保证。
   - 除了事务提交时，还有其他刷盘时机：如master thread每秒刷盘一次redo log等，这样的好处是不一定要等到commit时刷盘，commit速度大大加快。



##### MVCC



RR解决脏读、不可重复读、幻读等问题，使用的是MVCC：MVCC全称Multi-Version Concurrency Control，即多版本的并发控制协议。下面的例子很好的体现了MVCC的特点：在同一时刻，不同的事务读取到的数据可能是不同的(即多版本)——在T5时刻，事务A和事务C可以读取到不同版本的数据。
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020081723594270.png#pic_center)
MVCC最大的优点是读不加锁，因此读写不冲突，并发性能好。InnoDB实现MVCC，多个版本的数据可以共存，主要基于以下技术及数据结构：

1. 隐藏列：InnoDB中每行数据都有隐藏列，隐藏列中包含了本行数据的事务id、指向undo log的指针等。
2. 基于undo log的版本链：前面说到每行数据的隐藏列中包含了指向undo log的指针，而每条undo log也会指向更早版本的undo log，从而形成一条版本链。
3. ReadView：通过隐藏列和版本链，MySQL可以将数据恢复到指定版本；但是具体要恢复到哪个版本，则需要根据ReadView来确定。所谓ReadView，是指事务（记做事务A）在某一时刻给整个事务系统（trx_sys）打快照，之后再进行读操作时，会将读取到的数据中的事务id与trx_sys快照比较，从而判断数据对该ReadView是否可见，即对事务A是否可见。

trx_sys中的主要内容，以及判断可见性的方法如下：

- low_limit_id：表示生成ReadView时系统中应该分配给下一个事务的id。如果数据的事务id大于等于low_limit_id，则对该ReadView不可见。
- up_limit_id：表示生成ReadView时当前系统中活跃的读写事务中最小的事务id。如果数据的事务id小于up_limit_id，则对该ReadView可见。
- rw_trx_ids：表示生成ReadView时当前系统中活跃的读写事务的事务id列表。如果数据的事务id在low_limit_id和up_limit_id之间，则需要判断事务id是否在rw_trx_ids中：如果在，说明生成ReadView时事务仍在活跃中，因此数据对ReadView不可见；如果不在，说明生成ReadView时事务已经提交了，因此数据对ReadView可见。

下面以RR隔离级别为例，结合前文提到的几个问题分别说明。

1. 脏读
   ![在这里插入图片描述](https://img-blog.csdnimg.cn/20200818000127960.png#pic_center)
   当事务A在T3时刻读取zhangsan的余额前，会生成ReadView，由于此时事务B没有提交仍然活跃，因此其事务id一定在ReadView的rw_trx_ids中，因此根据前面介绍的规则，事务B的修改对ReadView不可见。接下来，事务A根据指针指向的undo log查询上一版本的数据，得到zhangsan的余额为100。这样事务A就避免了脏读。

2. 不可重复读
   ![在这里插入图片描述](https://img-blog.csdnimg.cn/20200818000211273.png#pic_center)
   当事务A在T2时刻读取zhangsan的余额前，会生成ReadView。此时事务B分两种情况讨论，一种是如图中所示，事务已经开始但没有提交，此时其事务id在ReadView的rw_trx_ids中；一种是事务B还没有开始，此时其事务id大于等于ReadView的low_limit_id。无论是哪种情况，根据前面介绍的规则，事务B的修改对ReadView都不可见。

   当事务A在T5时刻再次读取zhangsan的余额时，会根据T2时刻生成的ReadView对数据的可见性进行判断，从而判断出事务B的修改不可见；因此事务A根据指针指向的undo log查询上一版本的数据，得到zhangsan的余额为100，从而避免了不可重复读。

3. 幻读
   ![在这里插入图片描述](https://img-blog.csdnimg.cn/20200818000236747.png#pic_center)
   MVCC避免幻读的机制与避免不可重复读非常类似。

   当事务A在T2时刻读取0<id<5的用户余额前，会生成ReadView。此时事务B分两种情况讨论，一种是如图中所示，事务已经开始但没有提交，此时其事务id在ReadView的rw_trx_ids中；一种是事务B还没有开始，此时其事务id大于等于ReadView的low_limit_id。无论是哪种情况，根据前面介绍的规则，事务B的修改对ReadView都不可见。

   当事务A在T5时刻再次读取0<id<5的用户余额时，会根据T2时刻生成的ReadView对数据的可见性进行判断，从而判断出事务B的修改不可见。因此对于新插入的数据lisi(id=2)，事务A根据其指针指向的undo log查询上一版本的数据，发现该数据并不存在，从而避免了幻读。

**扩展**

前面介绍的MVCC，是RR隔离级别下“非加锁读”实现隔离性的方式。下面是一些简单的扩展。

1. 读已提交（RC）隔离级别下的非加锁读

   RC与RR一样，都使用了MVCC，其主要区别在于：

   RR是在事务开始后第一次执行select前创建ReadView，直到事务提交都不会再创建。根据前面的介绍，RR可以避免脏读、不可重复读和幻读。

   RC每次执行select前都会重新建立一个新的ReadView，因此如果事务A第一次select之后，事务B对数据进行了修改并提交，那么事务A第二次select时会重新建立新的ReadView，因此事务B的修改对事务A是可见的。因此RC隔离级别可以避免脏读，但是无法避免不可重复读和幻读。

2. 加锁读与next-key lock

   按照是否加锁，MySQL的读可以分为两种：

   一种是非加锁读，也称作快照读、一致性读，使用普通的select语句，这种情况下使用MVCC避免了脏读、不可重复读、幻读，保证了隔离性。

   另一种是加锁读，查询语句有所不同，如下所示：

   ```c
   #共享锁读取
   select...lock in share mode
   #排它锁读取
   select...for update
   1234
   ```

   加锁读在查询时会对查询的数据加锁（共享锁或排它锁）。由于锁的特性，当某事务对数据进行加锁读后，其他事务无法对数据进行写操作，因此可以避免脏读和不可重复读。而避免幻读，则需要通过next-key lock。**next-key lock是行锁的一种，实现相当于record lock(记录锁) + gap lock(间隙锁)；其特点是不仅会锁住记录本身(record lock的功能)**，还会锁定一个范围(gap lock的功能)。因此，加锁读同样可以避免脏读、不可重复读和幻读，保证隔离性。



概括来说，InnoDB实现的RR，通过锁机制（包含next-key lock）、MVCC（包括数据的隐藏列、基于undo log的版本链、ReadView）等，实现了一定程度的隔离性，可以满足大多数场景的需要。

不过需要说明的是，RR虽然避免了幻读问题，但是毕竟不是Serializable，不能保证完全的隔离，下面是两个例子：

第一个例子，如果在事务中第一次读取采用非加锁读，第二次读取采用加锁读，则如果在两次读取之间数据发生了变化，两次读取到的结果不一样，因为加锁读时不会采用MVCC。

第二个例子，如下所示，大家可以自己验证一下。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200818000413714.png#pic_center)





##### 一致性

1. 基本概念

一致性是指事务执行结束后，数据库的完整性约束没有被破坏，事务执行的前后都是合法的数据状态。数据库的完整性约束包括但不限于：实体完整性（如行的主键存在且唯一）、列完整性（如字段的类型、大小、长度要符合要求）、外键约束、用户自定义完整性（如转账前后，两个账户余额的和应该不变）。

2. 实现

可以说，一致性是事务追求的最终目标：前面提到的原子性、持久性和隔离性，都是为了保证数据库状态的一致性。此外，除了数据库层面的保障，一致性的实现也需要应用层面进行保障。

实现一致性的措施包括：

- 保证原子性、持久性和隔离性，如果这些特性无法保证，事务的一致性也无法保证
- 数据库本身提供保障，例如不允许向整形列插入字符串值、字符串长度不能超过列的限制等
- 应用层面进行保障，例如如果转账操作只扣除转账者的余额，而没有增加接收者的余额，无论数据库实现的多么完美，也无法保证状态的一致



下面总结一下ACID特性及其实现原理：

- 原子性：语句要么全执行，要么全不执行，是事务最核心的特性，事务本身就是以原子性来定义的；实现主要基于undo log
- 持久性：保证事务提交后不会因为宕机等原因导致数据丢失；实现主要基于redo log
- 隔离性：保证事务执行尽可能不受其他事务影响；InnoDB默认的隔离级别是RR，RR的实现主要基于锁机制（包含next-key lock）、MVCC（包括数据的隐藏列、基于undo log的版本链、ReadView）
- 一致性：事务追求的最终目标，一致性的实现既需要数据库层面的保障，也需要应用层面的保障



##### **事务的隔离级别**；

​	概念：多个事务之间隔离的，相互独立。但是如果多个事务操作同一批数据，则会引发一些问题，设置不同的隔离级别捷克语解决这些问题。

存在问题：

1. 脏读：一个事务，读到另一个事务中没有提交的数据

  2. 不可重复读（虚读）：在同一个事务中，两次读取到的数据不同

  3. 幻读：一个事务操作（DML）数据表中所有记录，另一个事务添加了一条数据，则第一个事务查询不到自己的修改。

     是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行

     幻读仅专指**新插入的行**

隔离级别：

 1. read uncommitted：已读尚未提交

    产生的问题：脏读，不可重复读，幻读

    2. readcommitted：已读已提交

    产生的问题：不可重复读，幻读

    3. repeatable read：可重复读（Mysql默认）

    产生的问题：幻读

    4. serializable：串行化

    可以解决所有的问题

**注意：隔离级别从小到大安全性越来越高，但是效率越来越低**

数据库查询隔离级别：

​	*select @@tx_isolation；

数据库设置隔离级别：

​	set global transction isolation level 级别字符串；



#### 行锁与表锁：

​		**锁是计算机协调多个进程或纯线程并发访问某一资源的机制。**在数据库中，除传统的计算资源（CPU、RAM、I/O）的争用以外，数据也是一种供许多用户共享的资源。如何保证数据并发访问的一致性、有效性是所在有数据库必须解决的一个问题，锁冲突也是影响数据库并发访问性能的一个重要因素。从这个角度来说，锁对数据库而言显得尤其重要，也更加复杂。

​		相对其他数据库而言，MySQL的锁机制比较简单，其最显著的特点是不同的存储引擎支持不同的锁机制。

MySQL大致可归纳为以下3种锁：

- 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。
- 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。
- 页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般

表锁：顾名思义就是对整张表进行加锁，同一时刻整张表所有记录都被霸占，虽然不会出现死锁问题但是锁冲突高堵塞高，并发低。

行锁：很明显只对某一行进行加锁，这样表的其余行并不会被占用，冲突低，并发高，但是死锁很可能出现。

锁冲突：竞争资源已经被持有，按顺序执行。
死锁：就是你请求的被别人锁住，被人请求被你锁住了，对方都没法进行下去。

在mysql 的 InnoDB引擎支持行锁，与Oracle不同，mysql的行锁是通过索引加载的，即是行锁是加在索引响应的行上的，要是对应的SQL语句没有走索引，则会全表扫描，行锁则无法实现，取而代之的是表锁。



### **InnoDB的行锁模式及加锁方法**

InnoDB实现了以下两种类型的行锁。

- 共享锁（s）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。
- 排他锁（Ｘ）：允许获取排他锁的事务更新数据，阻止其他事务取得相同的数据集共享读锁和排他写锁。

另外，为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁。

意向共享锁（IS）：事务打算给数据行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。

意向排他锁（IX）：事务打算给数据行加排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。

#### **InnoDB行锁模式兼容性列表**

| 当前锁模式/是否兼容/请求锁模式 | X    | IX   | S    | IS   |
| ------------------------------ | ---- | ---- | ---- | ---- |
| X                              | 冲突 | 冲突 | 冲突 | 冲突 |
| IX                             | 冲突 | 兼容 | 冲突 | 兼容 |
| S                              | 冲突 | 冲突 | 兼容 | 兼容 |
| IS                             | 冲突 | 兼容 | 兼容 | 兼容 |

 

  如果一个事务请求的锁模式与当前的锁兼容，InnoDB就请求的锁授予该事务；反之，如果两者不兼容，该事务就要等待锁释放。

  意向锁是InnoDB自动加的，不需用户干预。对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及及数据集加排他锁（Ｘ）；对于普通SELECT语句，InnoDB不会加任何锁；事务可以通过以下语句显示给记录集加共享锁或排锁。

共享锁（Ｓ）：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE

排他锁（X）：SELECT * FROM table_name WHERE ... FOR UPDATE

  用SELECT .. IN SHARE MODE获得共享锁，主要用在需要数据依存关系时确认某行记录是否存在，并确保没有人对这个记录进行UPDATE或者DELETE操作。但是如果当前事务也需要对该记录进行更新操作，则很有可能造成死锁，对于锁定行记录后需要进行更新操作的应用，应该使用SELECT ... FOR UPDATE方式获取排他锁。

  

 

### **InnoDB行锁实现方式**

  InnoDB行锁是通过索引上的索引项来实现的，这一点ＭySQL与Oracle不同，后者是通过在数据中对相应数据行加锁来实现的。InnoDB这种行锁实现特点意味者：只有通过索引条件检索数据，InnoDB才会使用行级锁，否则，InnoDB将使用表锁！

  在实际应用中，要特别注意InnoDB行锁的这一特性，不然的话，可能导致大量的锁冲突，从而影响并发性能。

  

读锁：允许其他线程上读锁，但是不允许上写锁。

```
lock in share mode
如： select * from user where user_name='wzzf' lock in share mode
```


写锁：不允许其他线程上任何锁。


写锁：不允许其他线程上任何锁。

```
for update
如：select * from user where user_name='wzzf' for update
```


下面给出测试例子：
先开个读锁


下面给出测试例子：
先开个读锁

```
BEGIN;
SELECT * from tyg_consumer_chain_sell_order o where o.consumer_chain_order_number=26911523448454 lock in SHARE MODE
```



再开个读锁，读同一行

```
BEGIN;
SELECT * from tyg_consumer_chain_sell_order o where o.consumer_chain_order_number=26911523448454 lock in SHARE MODE
```


两个都查询出来了，但是再开个写锁就一直没有停止，说明是被锁住了，后来停止报

Lock wait timeout exceeded; try restarting transaction
证明了读锁可以共享读，其他线程不能加写锁。

写锁就不再赘述，开启了写锁之后其他线程连加读锁都不允许。

然后处理读其他的都是默认加写锁的

行锁必须要索引才能实现，否则会自动锁全表，两个事务可以用同一个索引，下面给出例子：
读锁由于不排除其他线程再加读锁比较难测试，所以下面用写锁测试，先测试没加所以字段进行加锁，对不同记录进行加锁，如果都加锁成功说明是加了行锁，反之则是默认锁全表。

下面的consumer_chain_order_number是不会重复的，但没有索引

```
-- 马上显示查询结果
BEGIN;
SELECT * from tyg_consumer_chain_sell_order o where o.consumer_chain_order_number=26911523448454 for update

```

```
--一直没有结束知道等待超时
BEGIN;
SELECT * from tyg_consumer_chain_sell_order o where o.consumer_chain_order_number=55181523448554 for update

```

说明了默认锁全表，接下来试下有索引的字段，这个表的主键

说明了默认锁全表，接下来试下有索引的字段，这个表的主键

```
BEGIN;
SELECT * from tyg_consumer_chain_sell_order o where o.consumer_chain_order_id=1 for update

```



```
BEGIN;
SELECT * from tyg_consumer_chain_sell_order o where o.consumer_chain_order_id=2 for update
```


都查询出来了结果，说明锁住了行


都查询出来了结果，说明锁住了行

行锁为什么需要索引
有索引就能快速定位这个行，没有索引数据库也没不能及时锁住行需要全表扫描，为了安全只能把整个表锁住。




### 间隙锁（Next-Key锁）

  当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙(GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁（Next-Key锁）。

  举例来说，假如emp表中只有101条记录，其empid的值分别是1,2,...,100,101，下面的SQL：

```
SELECT * FROM emp WHERE empid > 100 FOR UPDATE
```

  是一个范围条件的检索，InnoDB不仅会对符合条件的empid值为101的记录加锁，也会对empid大于101（这些记录并不存在）的“间隙”加锁。

  InnoDB使用间隙锁的目的，一方面是为了防止幻读，以满足相关隔离级别的要求，对于上面的例子，要是不使用间隙锁，如果其他事务插入了empid大于100的任何记录，那么本事务如果再次执行上述语句，就会发生幻读；另一方面，是为了满足其恢复和复制的需要。有关其恢复和复制对机制的影响，以及不同隔离级别下InnoDB使用间隙锁的情况。

  很显然，在使用范围条件检索并锁定记录时，InnoDB这种加锁机制会阻塞符合条件范围内键值的并发插入，这往往会造成严重的锁等待。因此，在实际开发中，尤其是并发插入比较多的应用，我们要尽量优化业务逻辑，尽量使用相等条件来访问更新数据，避免使用范围条件。

 

 

### **什么时候使用表锁**

  对于InnoDB表，在绝大部分情况下都应该使用行级锁，因为事务和行锁往往是我们之所以选择InnoDB表的理由。但在个别特殊事务中，也可以考虑使用表级锁。

- 第一种情况是：事务需要更新大部分或全部数据，表又比较大，如果使用默认的行锁，不仅这个事务执行效率低，而且可能造成其他事务长时间锁等待和锁冲突，这种情况下可以考虑使用表锁来提高该事务的执行速度。
- 第二种情况是：事务涉及多个表，比较复杂，很可能引起死锁，造成大量事务回滚。这种情况也可以考虑一次性锁定事务涉及的表，从而避免死锁、减少数据库因事务回滚带来的开销。

  当然，应用中这两种事务不能太多，否则，就应该考虑使用ＭyISAＭ表。

  在InnoDB下 ，使用表锁要注意以下两点。

  （１）使用LOCK TALBES虽然可以给InnoDB加表级锁，但必须说明的是，表锁不是由InnoDB存储引擎层管理的，而是由其上一层ＭySQL Server负责的，仅当autocommit=0、innodb_table_lock=1（默认设置）时，InnoDB层才能知道MySQL加的表锁，ＭySQL Server才能感知InnoDB加的行锁，这种情况下，InnoDB才能自动识别涉及表级锁的死锁；否则，InnoDB将无法自动检测并处理这种死锁。

  （２）在用LOCAK TABLES对InnoDB锁时要注意，要将AUTOCOMMIT设为0，否则ＭySQL不会给表加锁；事务结束前，不要用UNLOCAK TABLES释放表锁，因为UNLOCK TABLES会隐含地提交事务；COMMIT或ROLLBACK不能释放用LOCAK TABLES加的表级锁，必须用UNLOCK TABLES释放表锁，正确的方式见如下语句。

  例如，如果需要写表t1并从表t读，可以按如下做：

```
SET` `AUTOCOMMIT=0;``LOCAK TABLES t1 WRITE, t2 ``READ``, ...;``[do something ``with` `tables t1 ``and` `here];``COMMIT``;``UNLOCK TABLES;
```

 

### **关于死锁**

  在InnoDB中，除单个SQL组成的事务外，锁是逐步获得的，这就决定了InnoDB发生死锁是可能的。

  发生死锁后，InnoDB一般都能自动检测到，并使一个事务释放锁并退回，另一个事务获得锁，继续完成事务。但在涉及外部锁，或涉及表锁的情况下，InnoDB并不能完全自动检测到死锁，这需要通过设置锁等待超时参数innodb_lock_wait_timeout来解决。需要说明的是，这个参数并不是只用来解决死锁问题，在并发访问比较高的情况下，如果大量事务因无法立即获取所需的锁而挂起，会占用大量计算机资源，造成严重性能问题，甚至拖垮数据库。我们通过设置合适的锁等待超时阈值，可以避免这种情况发生。

  通常来说，死锁都是应用设计的问题，通过调整业务流程、数据库对象设计、事务大小、以及访问数据库的SQL语句，绝大部分都可以避免。下面就通过实例来介绍几种避免死锁的常用方法。

  （１）在应用中，如果不同的程序会并发存取多个表，应尽量约定以相同的顺序为访问表，这样可以大大降低产生死锁的机会。如果两个session访问两个表的顺序不同，发生死锁的机会就非常高！但如果以相同的顺序来访问，死锁就可能避免。

  （２）在程序以批量方式处理数据的时候，如果事先对数据排序，保证每个线程按固定的顺序来处理记录，也可以大大降低死锁的可能。

  （３）在事务中，如果要更新记录，应该直接申请足够级别的锁，即排他锁，而不应该先申请共享锁，更新时再申请排他锁。

  （４）在REPEATEABLE-READ隔离级别下，如果两个线程同时对相同条件记录用SELECT...ROR UPDATE加排他锁，在没有符合该记录情况下，两个线程都会加锁成功。程序发现记录尚不存在，就试图插入一条新记录，如果两个线程都这么做，就会出现死锁。这种情况下，将隔离级别改成READ COMMITTED，就可以避免问题。

  （５）当隔离级别为READ COMMITED时，如果两个线程都先执行SELECT...FOR UPDATE，判断是否存在符合条件的记录，如果没有，就插入记录。此时，只有一个线程能插入成功，另一个线程会出现锁等待，当第１个线程提交后，第２个线程会因主键重出错，但虽然这个线程出错了，却会获得一个排他锁！这时如果有第３个线程又来申请排他锁，也会出现死锁。对于这种情况，可以直接做插入操作，然后再捕获主键重异常，或者在遇到主键重错误时，总是执行ROLLBACK释放获得的排他锁。

 

  尽管通过上面的设计和优化等措施，可以大减少死锁，但死锁很难完全避免。因此，在程序设计中总是捕获并处理死锁异常是一个很好的编程习惯。

  如果出现死锁，可以用SHOW INNODB STATUS命令来确定最后一个死锁产生的原因和改进措施。



对于**InnoDB**表，主要有以下几点

  （１）InnoDB的行锁是基于索引实现的，如果不通过索引访问数据，InnoDB会使用表锁。

  （２）InnoDB间隙锁机制，以及InnoDB使用间隙锁的原因。

  （３）在不同的隔离级别下，InnoDB的锁机制和一致性读策略不同。

  （４）ＭySQL的恢复和复制对InnoDB锁机制和一致性读策略也有较大影响。

  （５）锁冲突甚至死锁很难完全避免。

  在了解InnoDB的锁特性后，用户可以通过设计和SQL调整等措施减少锁冲突和死锁，包括：

- 尽量使用较低的隔离级别
- 精心设计索引，并尽量使用索引访问数据，使加锁更精确，从而减少锁冲突的机会。
- 选择合理的事务大小，小事务发生锁冲突的几率也更小。
- 给记录集显示加锁时，最好一次性请求足够级别的锁。比如要修改数据的话，最好直接申请排他锁，而不是先申请共享锁，修改时再请求排他锁，这样容易产生死锁。
- 不同的程序访问一组表时，应尽量约定以相同的顺序访问各表，对一个表而言，尽可能以固定的顺序存取表中的行。这样可以大减少死锁的机会。
- 尽量用相等条件访问数据，这样可以避免间隙锁对并发插入的影响。
- 不要申请超过实际需要的锁级别；除非必须，查询时不要显示加锁。
- 对于一些特定的事务，可以使用表锁来提高处理速度或减少死锁的可能。



#### DCL：

​	sql分类：

​	1. DDL：操作数据库和表

     		2. DML：增删改表中数据
                 		3. DQL：查询表中数据
                   		4. DCL：管理用户，授权

DBA：数据库管理员

DCL：管理用户，授权

 1. 管理用户

    1. 添加用户：

       语法：CREATE USER  '用户名'@‘主机名’ IDENTIFIED BY '密码'；

    2. 删除用户：

       语法：DROP USER  '用户名'@‘主机名’ ；

    3. 修改用户密码：

    4. 查询用户：

       1. 切换到mysql数据库

          use mysql；



### **mysql 常用的引擎**

5.5之前为MyISAM，5.5之后为InnoDB。

**1、InnoDB**

InnoDB是默认的数据库存储引擎，他的主要特点有：

（1）可以通过自动增长列，方法是auto_increment。

（2）支持事务。默认的事务隔离级别为可重复度，通过MVCC（并发版本控制）来实现的。

（3）使用的锁粒度为行级锁，可以支持更高的并发；

（4）支持外键约束；外键约束其实降低了表的查询速度，但是增加了表之间的耦合度。

（5）配合一些热备工具可以支持在线热备份；

（6）在InnoDB中存在着缓冲管理，通过缓冲池，将索引和数据全部缓存起来，加快查询的速度；

（7）对于InnoDB类型的表，其数据的物理组织形式是聚簇表。所有的数据按照主键来组织。数据和索引放在一块，都位于B+数的叶子节点上；

当然InnoDB的存储表和索引也有下面两种形式：

（1）使用共享表空间存储：所有的表和索引存放在同一个表空间中。

（2）使用多表空间存储：表结构放在frm文件，数据和索引放在IBD文件中。分区表的话，每个分区对应单独的IBD文件，分区表的定义可以查看我的其他文章。使用分区表的好处在于提升查询效率。

对于InnoDB来说，最大的特点在于支持事务。但是这是以损失效率来换取的。



**2、MyISAM**

使用这个存储引擎，每个MyISAM在磁盘上存储成三个文件。

（1）frm文件：存储表的定义数据

（2）MYD文件：存放表具体记录的数据

（3）MYI文件：存储索引

frm和MYI可以存放在不同的目录下。MYI文件用来存储索引，但仅保存记录所在页的指针，索引的结构是B+树结构。下面这张图就是MYI文件保存的机制：

![img](https://pics1.baidu.com/feed/f9198618367adab4d0c6e43a2953bb1a8601e4c7.jpeg?token=589182b3bf01e1fc9952977a8bedd815&s=E07C2072511FE1CC18C4DDCA020060B1)

从这张图可以发现，这个存储引擎通过MYI的B+树结构来查找记录页，再根据记录页查找记录。并且支持全文索引、B树索引和数据压缩。

支持数据的类型也有三种：

（1）静态固定长度表

这种方式的优点在于存储速度非常快，容易发生缓存，而且表发生损坏后也容易修复。缺点是占空间。这也是默认的存储格式。

（2）动态可变长表

优点是节省空间，但是一旦出错恢复起来比较麻烦。

（3）压缩表

上面说到支持数据压缩，说明肯定也支持这个格式。在数据文件发生错误时候，可以使用check table工具来检查，而且还可以使用repair table工具来恢复。

有一个重要的特点那就是不支持事务，但是这也意味着他的存储速度更快，如果你的读写操作允许有错误数据的话，只是追求速度，可以选择这个存储引擎。



**3、Memory**

将数据存在内存，为了提高数据的访问速度，每一个表实际上和一个磁盘文件关联。文件是frm。

（1）支持的数据类型有限制，比如：不支持TEXT和BLOB类型，对于字符串类型的数据，只支持固定长度的行，VARCHAR会被自动存储为CHAR类型；

（2）支持的锁粒度为表级锁。所以，在访问量比较大时，表级锁会成为MEMORY存储引擎的瓶颈；

（3）由于数据是存放在内存中，一旦服务器出现故障，数据都会丢失；

（4）查询的时候，如果有用到临时表，而且临时表中有BLOB，TEXT类型的字段，那么这个临时表就会转化为MyISAM类型的表，性能会急剧降低；

（5）默认使用hash索引。

（6）如果一个内部表很大，会转化为磁盘表。



##### **mysql 的行锁和表锁**

MyISAM 只支持表锁，InnoDB 支持表锁和行锁，默认为行锁。

- 表级锁：开销小，加锁快，不会出现死锁。锁定粒度大，发生锁冲突的概率最高，并发量最低。
- 行级锁：开销大，加锁慢，会出现死锁。锁力度小，发生锁冲突的概率小，并发度最高



**乐观锁和悲观锁？**

- 乐观锁：每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在**提交更新**的时候会判断一下在此期间别人有没有去更新这个数据。
- 悲观锁：每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻止，直到这个锁被释放



**mysql 问题排查都有哪些手段？**

- 使用 show processlist 命令查看当前所有连接信息。
- 使用 explain 命令查询 SQL 语句执行计划。
- 开启慢查询日志，查看慢查询的 SQL



**如何做 mysql 的性能优化？**

- 为搜索字段创建索引。
- 避免使用 select *，列出需要查询的字段。
- 垂直分割分表。
- 选择正确的存储引擎



#### Mysql常见面试题

**1.什么是关系型数据库？谈谈你对 MySQL 的认识。**

这是一道基础题，考察面试者对数据库的了解程度，一般可以简单讲下自己的认知，有条理即可。比如：

关系型数据库是指采用了关系模型来组织数据的数据库，其以行和列的形式存储数据。关系型数据库最大的特点是支持事务。常见的关系型数据库有 MySQL、Oracle、SQLServer 等。MySQL 是当下最流行的开源数据库。由于其体积小、速度快、总体拥有成本低，尤其是开放源码这一特点，使得很多公司都采用 MySQL 数据库以降低成本，目前被广泛地应用在 Internet 上的中小型网站中，尤其适用于 OLTP 领域



**2.MySQL 常见的存储引擎有哪些，有什么区别？**

常见的几种存储引擎：

- **InnoDB**: MySQL 默认的存储引擎，支持事务、MVCC、外键、行级锁和自增列。
- **MyISAM**: 支持全文索引、压缩、空间函数、表级锁，不支持事务，插入速度快。
- **Memory**: 数据都在内存中，数据的处理速度快，但是安全性不高。
- **ARCHIVE**: 常用于历史归档表，占用空间小，数据不能更新删除。



InnoDB 与 MyISAM 引擎的几点区别：



- InnoDB 支持事务，MyISAM 不支持事务。
- InnoDB 支持外键，而 MyISAM 不支持。
- InnoDB 不支持全文索引，而 MyISAM 支持。
- InnoDB 是聚簇索引，MyISAM 是非聚簇索引。
- InnoDB 不保存表的具体行数，而 MyISAM 用一个变量保存了整个表的行数。
- InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁。
- 存储结构不同，MyISAM 表分为 frm MYD MYI 三个，InnoDB 一般分为 frm ibd 两个。



**3.描述下 MySQL 基础架构。**



这个问题考察面试者对 MySQL 架构的了解，和『一条 select 语句执行流程』问题相似。



![图片](https://mmbiz.qpic.cn/mmbiz_png/4toJlb3BpXvmNdibiakhL0xd3dP3Q41bf6ibyLBuBKE3ZcuJFN9dibVN2FCWmBhRomENIakpmBMktjiay29r5BYW8UQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

MySQL的逻辑架构图(来源:《MySQL实战45讲》)



MySQL的逻辑架构主要分为3层：



1. 第一层：对客户端的连接处理、安全认证、授权等，每个客户端连接都会在服务端拥有一个线程，每个连接发起的查询都会在对应的单独线程中执行。
2. 第二层：MySQL的核心服务功能层，包括查询解析、分析、查询缓存、内置函数、存储过程、触发器、视图等，select操作会先检查是否命中查询缓存，命中则直接返回缓存数据，否则解析查询并创建对应的解析树。
3. 第三层：存储引擎，负责数据的存储和提取，MySQL服务器通过API与存储引擎通信，屏蔽了各种引擎之间的差异，常见的存储引擎有：InnoDB、MyISAM。



一条 select 语句执行流程：



- 客户端通过**连接器**与 MySQL 服务器建立连接，并获取了用户的读写权限，然后提交查询语句。
- 首先 MySQL 会在**查询缓存**中对提交的语句进行查询，如果命中且用户对表有操作权限，会直接返回查询缓存中查询结果作为本次查询的结果，查询到此结束。
- 如果查询缓存未命中，会来到**分析器**，分析器会解析语句并检查其合法性。如果语句不符合 MySQL 的语法规范，执行器会报错，查询到此结束。
- 若语句合法，会来到**优化器**，优化器会为 SQL 语句选择最优的执行计划。
- 最后来到**执行器**，如果用户对表有操作权限，执行器会调用存储引擎提供的接口来执行 SQL 语句，然后将查询结果返回给客户端，查询到此结束。



**4.说说常用的几种字段类型。**



这个问题考察面试者对 MySQL 字段类型的了解程度，可以延伸出很多小问题，例如 char 与 varchar 的区别。



常用的字段类型分类：



数值型：



![图片](https://mmbiz.qpic.cn/mmbiz_png/4toJlb3BpXvmNdibiakhL0xd3dP3Q41bf6rUYKNkqlFCxx57B52sMxxHNoKCFNwpV76Stibp4IGSVibibAichwticN3Kg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



字符串类型：



![图片](https://mmbiz.qpic.cn/mmbiz_png/4toJlb3BpXvmNdibiakhL0xd3dP3Q41bf6O0ias7YiaeN5fpojyq0IK4EHB9IyCbT7CGTXicmC6CnibNtOrpsQspdcPw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



日期和时间类型：



![图片](https://mmbiz.qpic.cn/mmbiz_png/4toJlb3BpXvmNdibiakhL0xd3dP3Q41bf6eJl7rw6fmuVcsfZAfSZmBejmHgc7p7nseV3wCTQjbP5wMy601ibelSQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



int(M)中的 M 代表最大显示宽度，"最大显示宽度"我们第一反应是该字段的值最大能允许存放的值的宽度，以为我们建了int(1)，就不能存放数据10了, 其实不是这个意思，int(5)和int(10)可存储的范围一样。



CHAR类型是定长的，MySQL总是根据定义的字符串长度分配足够的空间。当保存CHAR值时，在它们的右边填充空格以达到指定的长度，当检索到CHAR值时，尾部的空格被删除掉。VARCHAR类型用于存储可变长字符串，存储时，如果字符没有达到定义的位数，也不会在后面补空格。char(M) 与 varchar(M)中的的 M 都表示保存的最大字符数，单个字母、数字、中文等都是占用一个字符。



**5.讲讲索引的作用及结构及使用规范。**



关于索引，能有好多好多问题，可能几篇文章也写不明白。简单分享下这类问题的回答：



索引的目的在于提高查询效率。可以类比字典中的目录，查找字典内容时可以根据目录查找到数据的存放位置，然后直接获取即可。索引是表的目录，在查找内容之前可以先在目录中查找索引位置，以此快速定位查询数据。



InnoDB 引擎下，主要使用的是 B+Tree 索引，每个索引其实都是一颗B+树，B+树是为了磁盘及其他存储辅助设备而设计的一种平衡查找树(不是二叉树)，在B+树中，所有的数据都在叶子节点，且每一个叶子节点都带有指向下一个节点的指针，形成了一个有序的链表。



从物理存储角度来看，InnoDB 索引可分为聚簇索引（clustered index）和二级索引（secondary index）或辅助索引。聚簇索引的叶子节点存的是整行数据，当某条查询使用的是聚簇索引时，只需要扫描聚簇索引一颗B+树即可得到所需记录，如果想通过二级索引来查找完整的记录的话，需要通过回表操作，也就是在通过二级索引找到主键值之后再到聚簇索引中查找完整的记录。



索引的优点显而易见是可以加速查询，但创建索引也是有代价的。首先每建立一个索引都要为它建立一棵B+树，会占用额外的存储空间；其次当对表中的数据进行增加、删除、修改时，索引也需要动态的维护，降低了数据的维护速度。所以，索引的创建及使用时有原则的，一般只为用于搜索、排序、分组、连接的列创建索引，选择性差的列尽量不创建索引。



1、如果我们定义了主键(PRIMARY KEY)，那么InnoDB会选择主键作为聚集索引。

如果没有显式定义主键，则InnoDB会选择第一个不包含有NULL值的唯一索引作为主键索引。

如果也没有这样的唯一索引，则InnoDB会选择内置6字节长的ROWID作为隐含的聚集索引(ROWID随着行记录的写入而主键递增，这个ROWID不像ORACLE的ROWID那样可引用，是隐含的)。

2、数据记录本身被存于主索引（一颗B+Tree）的叶子节点上，这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放

因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）

3、如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页

4、如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置

此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销

同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。

**二、为什么使用数据索引能提高效率**

1. 数据索引的存储是有序的
2. 在有序的情况下，通过索引查询一个数据是无需遍历索引记录的
3. 极端情况下，数据索引的查询效率为二分法查询效率，趋近于 log2(N)

**三、B+树索引和哈希索引的区别**

B+树是一个平衡的多叉树，从根节点到每个叶子节点的高度差值不超过1，而且同层级的节点间有指针相互链接，是有序的，如下图：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/UtWdDgynLdYnMu5lfXNAYzW0PPSOB8Pss8E5IlpSXicQbuCj5p3fN1vGtKkdUgeZ4IvYBx4IlFMLI4peDFvTV2w/640?wx_fmt=jpeg)

哈希索引就是采用一定的哈希算法，把键值换算成新的哈希值，检索时不需要类似B+树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可,是无序的，如下图所示：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/UtWdDgynLdYnMu5lfXNAYzW0PPSOB8PsAdicCricepbjicRIBIOlKdDPWlHroEiaYVgdDgicMMWbsuIlmmA4kOEVVog/640?wx_fmt=jpeg)

**四、哈希索引的优势：**

**等值查询，**哈希索引具有绝对优势（前提是：没有大量重复键值，如果大量重复键值时，哈希索引的效率很低，因为存在所谓的哈希碰撞问题。）

**五、哈希索引不适用的场景：**

1. 不支持范围查询
2. 不支持索引完成排序
3. 不支持联合索引的最左前缀匹配规则

通常，B+树索引结构适用于绝大多数场景，像下面这种场景用哈希索引才更有优势：

在HEAP表中，如果存储的数据重复度很低（也就是说基数很大），对该列数据以等值查询为主，没有范围查询、没有排序的时候，特别适合采用哈希索引，例如这种SQL：

\# 仅等值查询

select id, name from table where name='李明'; 

而常用的 InnoDB 引擎中默认使用的是B+树索引，它会实时监控表上索引的使用情况。

如果认为建立哈希索引可以提高查询效率，则自动在内存中的“自适应哈希索引缓冲区”建立哈希索引（在InnoDB中默认开启自适应哈希索引）。

通过观察搜索模式，MySQL会利用index key的前缀建立哈希索引，如果一个表几乎大部分都在缓冲池中，那么建立一个哈希索引能够加快等值查询。

**注意**：在某些工作负载下，通过哈希索引查找带来的性能提升远大于额外的监控索引搜索情况和保持这个哈希表结构所带来的开销。

但某些时候，在负载高的情况下，自适应哈希索引中添加的read/write锁也会带来竞争，比如高并发的join操作。like操作和%的通配符操作也不适用于自适应哈希索引，可能要关闭自适应哈希索引。

**六、B树和B+树的区别**

1、B树，每个节点都存储key和data，所有节点组成这棵树，并且叶子节点指针为nul，叶子结点不包含任何关键字信息。

![img](https://mmbiz.qpic.cn/mmbiz_jpg/UtWdDgynLdYnMu5lfXNAYzW0PPSOB8PsvK1h1OM5xXxKeN8BDQXdyI3nFHQ5R0akaWtCh5m0OPv8cvARObgDicg/640?wx_fmt=jpeg)

2、B+树，所有的叶子结点中包含了全部关键字的信息，及指向含有这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大的顺序链接

所有的非终端结点可以看成是索引部分，结点中仅含有其子树根结点中最大（或最小）关键字。 (而B 树的非终节点也包含需要查找的有效信息)

![img](https://mmbiz.qpic.cn/mmbiz_jpg/UtWdDgynLdYnMu5lfXNAYzW0PPSOB8Psxxd09tVHZfEOicOXwrxzGFt5JibH6j44pxIpSC1ZePOFC0stO2rpBvyw/640?wx_fmt=jpeg)

**七、为什么说B+比B树更适合实际应用中操作系统的文件索引和数据库索引？**

1、B+的磁盘读写代价更低。

B+的内部结点并没有指向关键字具体信息的指针，因此其内部结点相对B树更小。

如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说IO读写次数也就降低了。

2、B+-tree的查询效率更加稳定。

由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。

**八、MySQL联合索引**

1、联合索引是两个或更多个列上的索引。

对于联合索引:Mysql从左到右的使用索引中的字段，一个查询可以只使用索引中的一部份，但只能是最左侧部分。

例如索引是key index (a,b,c). 可以支持a 、 a,b 、 a,b,c 3种组合进行查找，但不支持 b,c进行查找 .当最左侧字段是常量引用时，索引就十分有效。

2、利用索引中的附加列，您可以缩小搜索的范围，但使用一个具有两列的索引不同于使用两个单独的索引。

复合索引的结构与电话簿类似，人名由姓和名构成，电话簿首先按姓氏对进行排序，然后按名字对有相同姓氏的人进行排序。

如果您知道姓，电话簿将非常有用；如果您知道姓和名，电话簿则更为有用，但如果您只知道名不知道姓，电话簿将没有用处。

**九、什么情况下应不建或少建索引**

1、表记录太少

2、经常插入、删除、修改的表

3、数据重复且分布平均的表字段，假如一个表有10万行记录，有一个字段A只有T和F两种值，且每个值的分布概率大约为50%，那么对这种表A字段建索引一般不会提高数据库的查询速度。

4、经常和主字段一块查询但主字段索引值比较多的表字段

**十、什么是表分区？**

表分区，是指根据一定规则，将数据库中的一张表分解成多个更小的，容易管理的部分。从逻辑上看，只有一张表，但是底层却是由多个物理分区组成。

**十一、表分区与分表的区别**

**分表**：指的是通过一定规则，将一张表分解成多张不同的表。比如将用户订单记录根据时间成多个表。

**分表与分区的区别在于**：分区从逻辑上来讲只有一张表，而分表则是将一张表分解成多张表。

**十二、表分区有什么好处？**

1、**存储更多数据**。分区表的数据可以分布在不同的物理设备上，从而高效地利用多个硬件设备。和单个磁盘或者文件系统相比，可以存储更多数据

2、**优化查询**。在where语句中包含分区条件时，可以只扫描一个或多个分区表来提高查询效率；涉及sum和count语句时，也可以在多个分区上并行处理，最后汇总结果。

3、**分区表更容易维护**。例如：想批量删除大量数据可以清除整个分区。

4、**避免某些特殊的瓶颈**，例如InnoDB的单个索引的互斥访问，ext3问价你系统的inode锁竞争等。

**十三、分区表的限制因素**

1. 一个表最多只能有1024个分区
2. MySQL5.1中，分区表达式必须是整数，或者返回整数的表达式。在MySQL5.5中提供了非整数表达式分区的支持。
3. 如果分区字段中有主键或者唯一索引的列，那么多有主键列和唯一索引列都必须包含进来。即：分区字段要么不包含主键或者索引列，要么包含全部主键和索引列。
4. 分区表中无法使用外键约束
5. MySQL的分区适用于一个表的所有数据和索引，不能只对表数据分区而不对索引分区，也不能只对索引分区而不对表分区，也不能只对表的一部分数据分区。

**十四、如何判断当前MySQL是否支持分区？**

命令：show variables like '%partition%' 运行结果:

 

have_partintioning 的值为YES，表示支持分区。

**十五、MySQL支持的分区类型有哪些？**

1. **RANGE分区**： 这种模式允许将数据划分不同范围。例如可以将一个表通过年份划分成若干个分区
2. **LIST分区**： 这种模式允许系统通过预定义的列表的值来对数据进行分割。按照List中的值分区，与RANGE的区别是，range分区的区间范围值是连续的。
3. **HASH分区** ：这中模式允许通过对表的一个或多个列的Hash Key进行计算，最后通过这个Hash码不同数值对应的数据区域进行分区。例如可以建立一个对表主键进行分区的表。
4. **KEY分区** ：上面Hash模式的一种延伸，这里的Hash Key是MySQL系统产生的。

**十六、四种隔离级别**

1. **Serializable (串行化)**：可避免脏读、不可重复读、幻读的发生。
2. **Repeatable read (可重复读)**：可避免脏读、不可重复读的发生。
3. **Read committed (读已提交)**：可避免脏读的发生。
4. **Read uncommitted (读未提交)**：最低级别，任何情况都无法保证。

**十七、关于MVVC**

MySQL InnoDB存储引擎，实现的是基于多版本的并发控制协议——MVCC (Multi-Version Concurrency Control) 

**注**：与MVCC相对的，是基于锁的并发控制，Lock-Based Concurrency Control

**MVCC最大的好处**：读不加锁，读写不冲突。在读多写少的OLTP应用中，读写不冲突是非常重要的，极大的增加了系统的并发性能，现阶段几乎所有的RDBMS，都支持了MVCC。

1. LBCC：Lock-Based Concurrency Control，基于锁的并发控制

2. MVCC：Multi-Version Concurrency Control

   基于多版本的并发控制协议。纯粹基于锁的并发机制并发量低，MVCC是在基于锁的并发控制上的改进，主要是在读操作上提高了并发量。

**十八、在MVCC并发控制中，读操作可以分成两类：**

1. **快照读 (snapshot read)**：读取的是记录的可见版本 (有可能是历史版本)，不用加锁（共享读锁s锁也不加，所以不会阻塞其他事务的写）
2. **当前读 (current read)**：读取的是记录的最新版本，并且，当前读返回的记录，都会加上锁，保证其他事务不会再并发修改这条记录

**十九、行级锁定的优点：**

1、当在许多线程中访问不同的行时只存在少量锁定冲突。

2、回滚时只有少量的更改

3、可以长时间锁定单一的行。

**二十、行级锁定的缺点：**

1. 比页级或表级锁定占用更多的内存。
2. 当在表的大部分中使用时，比页级或表级锁定速度慢，因为你必须获取更多的锁。
3. 如果你在大部分数据上经常进行GROUP BY操作或者必须经常扫描整个表，比其它锁定明显慢很多。
4. 用高级别锁定，通过支持不同的类型锁定，你也可以很容易地调节应用程序，因为其锁成本小于行级锁定。

**二十一、MySQL优化**

1. 开启查询缓存，优化查询

2. explain你的select查询，这可以帮你分析你的查询语句或是表结构的性能瓶颈。EXPLAIN 的查询结果还会告诉你你的索引主键被如何利用的，你的数据表是如何被搜索和排序的

3. 当只要一行数据时使用limit 1，MySQL数据库引擎会在找到一条数据后停止搜索，而不是继续往后查少下一条符合记录的数据

4. 为搜索字段建索引

5. 使用 ENUM 而不是 VARCHAR。如果你有一个字段，比如“性别”，“国家”，“民族”，“状态”或“部门”，你知道这些字段的取值是有限而且固定的，那么，你应该使用 ENUM 而不是VARCHAR

6. Prepared StatementsPrepared Statements很像存储过程，是一种运行在后台的SQL语句集合，我们可以从使用 prepared statements 获得很多好处，无论是性能问题还是安全问题。

   Prepared Statements 可以检查一些你绑定好的变量，这样可以保护你的程序不会受到“SQL注入式”攻击

7. 垂直分表

8. 选择正确的存储引擎

**二十二、key和index的区别**

1. key 是数据库的物理结构，它包含两层意义和作用，一是约束（偏重于约束和规范数据库的结构完整性），二是索引（辅助查询用的）。包括primary key, unique key, foreign key 等
2. index是数据库的物理结构，它只是辅助查询的，它创建时会在另外的表空间（mysql中的innodb表空间）以一个类似目录的结构存储。索引要分类的话，分为前缀索引、全文本索引等；

**二十三、Mysql 中 MyISAM 和 InnoDB 的区别有哪些？**

**区别：**

1. InnoDB支持事务，MyISAM不支持

   对于InnoDB每一条SQL语言都默认封装成事务，自动提交，这样会影响速度，所以最好把多条SQL语言放在begin和commit之间，组成一个事务；

2. InnoDB支持外键，而MyISAM不支持。对一个包含外键的InnoDB表转为MYISAM会失败；

3. InnoDB是聚集索引，数据文件是和索引绑在一起的，必须要有主键，通过主键索引效率很高。

   但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此主键不应该过大，因为主键太大，其他索引也都会很大。

   而MyISAM是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。

4. InnoDB不保存表的具体行数，执行select count(*) from table时需要全表扫描。而MyISAM用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快；

5. Innodb不支持全文索引，而MyISAM支持全文索引，查询效率上MyISAM要高；

**如何选择：**

1. 是否要支持事务，如果要请选择innodb，如果不需要可以考虑MyISAM；
2. 如果表中绝大多数都只是读查询，可以考虑MyISAM，如果既有读写也挺频繁，请使用InnoDB
3. 系统奔溃后，MyISAM恢复起来更困难，能否接受；
4. MySQL5.5版本开始Innodb已经成为Mysql的默认引擎(之前是MyISAM)，说明其优势是有目共睹的，如果你不知道用什么，那就用InnoDB，至少不会差。

**二十四、数据库表创建注意事项**

**1、字段名及字段配制合理性**

- 剔除关系不密切的字段；
- 字段命名要有规则及相对应的含义（不要一部分英文，一部分拼音，还有类似a.b.c这样不明含义的字段）；
- 字段命名尽量不要使用缩写（大多数缩写都不能明确字段含义）；
- 字段不要大小写混用（想要具有可读性，多个英文单词可使用下划线形式连接）；
- 字段名不要使用保留字或者关键字；
- 保持字段名和类型的一致性；
- 慎重选择数字类型；
- 给文本字段留足余量；

**2、系统特殊字段处理及建成后建议**

- 添加删除标记（例如操作人、删除时间）；
- 建立版本机制；

**3、表结构合理性配置**

- 多型字段的处理，就是表中是否存在字段能够分解成更小独立的几部分（例如：人可以分为男人和女人）；
- 多值字段的处理，可以将表分为三张表，这样使得检索和排序更加有调理，且保证数据的完整性！

**4、其它建议**

- 对于大数据字段，独立表进行存储，以便影响性能（例如：简介字段）；
- 使用varchar类型代替char，因为varchar会动态分配长度，char指定长度是固定的；
- 给表创建主键，对于没有主键的表，在查询和索引定义上有一定的影响；
- 避免表字段运行为null，建议设置默认值（例如：int类型设置默认值为0）在索引查询上，效率立显；
- 建立索引，最好建立在唯一和非空的字段上，建立太多的索引对后期插入、更新都存在一定的影响（考虑实际情况来创建）；



2、说下mysql的索引有哪些吧，聚簇和非聚簇索引又是什么？
索引按照数据结构来说主要包含B+树和Hash索引。

假设我们有张表，结构如下：

create table user( id int(11) not null, age int(11) not null, primary key(id), key(age) );

B+树是左小右大的顺序存储结构，节点只包含id索引列，而叶子节点包含索引列和数据，这种数据和索引在一起存储的索引方式叫做聚簇索引，一张表只能有一个聚簇索引。假设没有定义主键，InnoDB会选择一个唯一的非空索引代替，如果没有的话则会隐式定义一个主键作为聚簇索引。

这是主键聚簇索引存储的结构，那么非聚簇索引的结构是什么样子呢？非聚簇索引(二级索引)保存的是主键id值，这一点和myisam保存的是数据地址是不同的。


最终，我们一张图看看InnoDB和Myisam聚簇和非聚簇索引的区别


3、那你知道什么是覆盖索引和回表吗？
覆盖索引指的是在一次查询中，如果一个索引包含或者说覆盖所有需要查询的字段的值，我们就称之为覆盖索引，而不再需要回表查询。

而要确定一个查询是否是覆盖索引，我们只需要explain sql语句看Extra的结果是否是“Using index”即可。

以上面的user表来举例，我们再增加一个name字段，然后做一些查询试试。

explain select * from user where age=1; //查询的name无法从索引数据获取 explain select id,age from user where age=1; //可以直接从索引获取


2、说下mysql的索引有哪些吧，聚簇和非聚簇索引又是什么？
索引按照数据结构来说主要包含B+树和Hash索引。

假设我们有张表，结构如下：

create table user( id int(11) not null, age int(11) not null, primary key(id), key(age) );

B+树是左小右大的顺序存储结构，节点只包含id索引列，而叶子节点包含索引列和数据，这种数据和索引在一起存储的索引方式叫做聚簇索引，一张表只能有一个聚簇索引。假设没有定义主键，InnoDB会选择一个唯一的非空索引代替，如果没有的话则会隐式定义一个主键作为聚簇索引。

这是主键聚簇索引存储的结构，那么非聚簇索引的结构是什么样子呢？非聚簇索引(二级索引)保存的是主键id值，这一点和myisam保存的是数据地址是不同的。


最终，我们一张图看看InnoDB和Myisam聚簇和非聚簇索引的区别


3、那你知道什么是覆盖索引和回表吗？
覆盖索引指的是在一次查询中，如果一个索引包含或者说覆盖所有需要查询的字段的值，我们就称之为覆盖索引，而不再需要回表查询。

而要确定一个查询是否是覆盖索引，我们只需要explain sql语句看Extra的结果是否是“Using index”即可。

以上面的user表来举例，我们再增加一个name字段，然后做一些查询试试。

explain select * from user where age=1; //查询的name无法从索引数据获取 explain select id,age from user where age=1; //可以直接从索引获取


你们数据量级多大？分库分表怎么做的？
首先分库分表分为垂直和水平两个方式，一般来说我们拆分的顺序是先垂直后水平。

垂直分库

基于现在微服务拆分来说，都是已经做到了垂直分库了。

垂直分表

如果表字段比较多，将不常用的、数据较大的等等做拆分。


水平分表

首先根据业务场景来决定使用什么字段作为分表字段(sharding_key)，比如我们现在日订单1000万，我们大部分的场景来源于C端，我们可以用user_id作为sharding_key，数据查询支持到最近3个月的订单，超过3个月的做归档处理，那么3个月的数据量就是9亿，可以分1024张表，那么每张表的数据大概就在100万左右。

比如用户id为100，那我们都经过hash(100)，然后对1024取模，就可以落到对应的表上了。


10、那分表后的ID怎么保证唯一性的呢？
因为我们主键默认都是自增的，那么分表之后的主键在不同表就肯定会有冲突了。有几个办法考虑：

设定步长，比如1-1024张表我们分别设定1-1024的基础步长，这样主键落到不同的表就不会冲突了。
分布式ID，自己实现一套分布式ID生成算法或者使用开源的比如雪花算法这种。
分表后不使用主键作为查询依据，而是每张表单独新增一个字段作为唯一主键使用，比如订单表订单号是唯一的，不管最终落在哪张表都基于订单号作为查询依据，更新也一样。
11、 分表后非sharding_key的查询怎么处理呢？
可以做一个mapping表，比如这时候商家要查询订单列表怎么办呢？不带user_id查询的话你总不能扫全表吧？所以我们可以做一个映射关系表，保存商家和用户的关系，查询的时候先通过商家查询到用户列表，再通过user_id去查询。
打宽表，一般而言，商户端对数据实时性要求并不是很高，比如查询订单列表，可以把订单表同步到离线（实时）数仓，再基于数仓去做成一张宽表，再基于其他如es提供查询服务。
数据量不是很大的话，比如后台的一些查询之类的，也可以通过多线程扫表，然后再聚合结果的方式来做。或者异步的形式也是可以的。
List<Callable<List<User>>> taskList = Lists.newArrayList(); for (int shardingIndex = 0; shardingIndex < 1024; shardingIndex++) { taskList.add(() -> (userMapper.getProcessingAccountList(shardingIndex))); } List<ThirdAccountInfo> list = null; try { list = taskExecutor.executeTask(taskList); } catch (Exception e) { //do something } public class TaskExecutor { public <T> List<T> executeTask(Collection<? extends Callable<T>> tasks) throws Exception { List<T> result = Lists.newArrayList(); List<Future<T>> futures = ExecutorUtil.invokeAll(tasks); for (Future<T> future : futures) { result.add(future.get()); } return result; } }
1
12、说说mysql主从同步怎么做的吧？
首先先了解mysql主从同步的原理

master提交完事务后，写入binlog
slave连接到master，获取binlog
master创建dump线程，推送binglog到slave
slave启动一个IO线程读取同步过来的master的binlog，记录到relay log中继日志中
slave再开启一个sql线程读取relay log事件并在slave执行，完成同步
slave记录自己的binglog

由于mysql默认的复制方式是异步的，主库把日志发送给从库后不关心从库是否已经处理，这样会产生一个问题就是假设主库挂了，从库处理失败了，这时候从库升为主库后，日志就丢失了。由此产生两个概念。
全同步复制

主库写入binlog后强制同步日志到从库，所有的从库都执行完成后才返回给客户端，但是很显然这个方式的话性能会受到严重影响。

半同步复制

和全同步不同的是，半同步复制的逻辑是这样，从库写入日志成功后返回ACK确认给主库，主库收到至少一个从库的确认就认为写操作完成。


#### [如何保证数据库与缓存的数据一致性](https://www.cnblogs.com/mengchunchen/p/10065351.html)


最经典的缓存+数据库读写的模式：cache aside pattern

**Cache Aside Pattern**

　　读的时候，先读缓存，缓存没有的话，那么就读数据库，然后取出数据后放入缓存，同时返回响应

　　更新的时候，先删除缓存，然后再更新数据库  （很多地方都说应该先更新数据库，再删缓存）

 

**为什么是删除缓存，而不是更新缓存呢？**

　　原因很简单，很多时候，复杂点的缓存的场景，因为缓存有的时候，不简单是数据库中直接取出来的值

　　比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据，并进行运算，才能计算出缓存最新的值的

　　更新缓存的代价是很高的

　　如果你频繁修改一个缓存涉及的多个表，那么这个缓存会被频繁的更新

　　但是问题在于，这个缓存到底会不会被频繁访问到？

　　举个例子，一个缓存涉及的表的字段，在1分钟内就修改了20次，或者是100次，那么缓存更新20次，100次; 但是这个缓存在1分钟内就被读取了1次，有大量的冷数据

　　实际上，如果你只是删除缓存的话，那么1分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低

　　每次数据过来，就只是删除缓存，然后修改数据库，如果这个缓存，在1分钟内只是被访问了1次，那么只有那1次，缓存是要被重新计算的，用缓存才去算缓存

　　其实删除缓存，而不是更新缓存，就是一个lazy计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算

 


**最初级的缓存不一致问题以及解决方案**

　　问题：先修改数据库，再删除缓存，如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据出现不一致

　　解决思路：先删除缓存，再修改数据库，如果删除缓存成功，修改数据库失败，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致

　　因为读的时候缓存没有，则读数据库中旧数据，然后更新到缓存中

 

**比较复杂的数据不一致问题分析**

　　数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改

　　一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中

　　数据变更的程序完成了数据库的修改，此时缓存是旧数据，数据库是新数据

 

**解决方案：更新与读取操作进行异步串行化**

　　更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个jvm内部的队列中

　　读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个jvm内部的队列中

　　一个队列对应一个工作线程

　　每个工作线程串行拿到对应的操作，然后一条一条的执行

　　这样的话，一个数据变更的操作，先执行，删除缓存，然后再去更新数据库，但是还没完成更新

　　此时如果一个读请求过来，读到了空的缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成

 

　　这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可

　　待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中

　　如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回; 如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值

![img](https://img2018.cnblogs.com/blog/720994/201812/720994-20181204171801930-539118551.png)

 

####  MySQL数据库优化的八种方式(经典必看)

1、选取最适用的字段属性

MySQL可以很好的支持大数据量的存取，但是一般说来，数据库中的表越小，在它上面执行的查询也就会越快。因此，在创建表的时候，为了获得更好的性能，我们可以将表中字段的宽度设得尽可能小。

例如，在定义邮政编码这个字段时，如果将其设置为CHAR(255),显然给数据库增加了不必要的空间，甚至使用VARCHAR这种类型也是多余的，因为CHAR(6)就可以很好的完成任务了。同样的，如果可以的话，我们应该使用MEDIUMINT而不是BIGIN来定义整型字段。

另外一个提高效率的方法是在可能的情况下，应该尽量把字段设置为NOTNULL，这样在将来执行查询的时候，数据库不用去比较NULL值。
对于某些文本字段，例如“省份”或者“性别”，我们可以将它们定义为ENUM类型。因为在MySQL中，ENUM类型被当作数值型数据来处理，而数值型数据被处理起来的速度要比文本类型快得多。这样，我们又可以提高数据库的性能。

2、使用连接（JOIN）来代替子查询(Sub-Queries)

MySQL从4.1开始支持SQL的子查询。这个技术可以使用SELECT语句来创建一个单列的查询结果，然后把这个结果作为过滤条件用在另一个查询中。例如，我们要将客户基本信息表中没有任何订单的客户删除掉，就可以利用子查询先从销售信息表中将所有发出订单的客户ID取出来，然后将结果传递给主查询，如下所示：

```
DELETEFROMcustomerinfo
WHERECustomerIDNOTin(SELECTCustomerIDFROMsalesinfo)
```

使用子查询可以一次性的完成很多逻辑上需要多个步骤才能完成的SQL操作，同时也可以避免事务或者表锁死，并且写起来也很容易。但是，有些情况下，子查询可以被更有效率的连接（JOIN）..替代。例如，假设我们要将所有没有订单记录的用户取出来，可以用下面这个查询完成：

```
SELECT*FROMcustomerinfo
WHERECustomerIDNOTin(SELECTCustomerIDFROMsalesinfo)
```

如果使用连接（JOIN）..来完成这个查询工作，速度将会快很多。尤其是当salesinfo表中对CustomerID建有索引的话，性能将会更好，查询如下：

```
SELECT*FROMcustomerinfo
LEFTJOINsalesinfoONcustomerinfo.CustomerID=salesinfo.CustomerID
WHEREsalesinfo.CustomerIDISNULL
```

连接（JOIN）..之所以更有效率一些，是因为MySQL不需要在内存中创建临时表来完成这个逻辑上的需要两个步骤的查询工作。

3、使用联合(UNION)来代替手动创建的临时表

MySQL从4.0的版本开始支持union查询，它可以把需要使用临时表的两条或更多的select查询合并的一个查询中。在客户端的查询会话结束的时候，临时表会被自动删除，从而保证数据库整齐、高效。使用union来创建查询的时候，我们只需要用UNION作为关键字把多个select语句连接起来就可以了，要注意的是所有select语句中的字段数目要想同。下面的例子就演示了一个使用UNION的查询。

```
SELECTName,PhoneFROMclientUNION
SELECTName,BirthDateFROMauthorUNION
SELECTName,SupplierFROMproduct
```

4、事务

尽管我们可以使用子查询（Sub-Queries）、连接（JOIN）和联合（UNION）来创建[各种各样](https://www.baidu.com/s?wd=各种各样&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)的查询，但不是所有的数据库操作都可以只用一条或少数几条SQL语句就可以完成的。更多的时候是需要用到一系列的语句来完成某种工作。但是在这种情况下，当这个语句块中的某一条语句运行出错的时候，整个语句块的操作就会变得不确定起来。设想一下，要把某个数据同时插入两个相关联的表中，可能会出现这样的情况：第一个表中成功更新后，数据库突然出现意外状况，造成第二个表中的操作没有完成，这样，就会造成数据的不完整，甚至会破坏数据库中的数据。要避免这种情况，就应该使用事务，它的作用是：要么语句块中每条语句都操作成功，要么都失败。换句话说，就是可以保持数据库中数据的一致性和完整性。事物以BEGIN关键字开始，COMMIT关键字结束。在这之间的一条SQL操作失败，那么，ROLLBACK命令就可以把数据库恢复到BEGIN开始之前的状态。

```
BEGIN; INSERTINTOsalesinfoSETCustomerID=14;UPDATEinventorySETQuantity=11WHEREitem='book';COMMIT;
```

事务的另一个重要作用是当多个用户同时使用相同的数据源时，它可以利用锁定数据库的方法来为用户提供一种安全的访问方式，这样可以保证用户的操作不被其它的用户所干扰。

5、锁定表

尽管事务是维护数据库完整性的一个非常好的方法，但却因为它的独占性，有时会影响数据库的性能，尤其是在很大的应用系统中。由于在事务执行的过程中，数据库将会被锁定，因此其它的用户请求只能暂时等待直到该事务结束。如果一个数据库系统只有少数几个用户来使用，事务造成的影响不会成为一个太大的问题；但假设有[成千上万](https://www.baidu.com/s?wd=成千上万&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)的用户同时访问一个数据库系统，例如访问一个电子商务网站，就会产生比较严重的响应延迟。

其实，有些情况下我们可以通过锁定表的方法来获得更好的性能。下面的例子就用锁定表的方法来完成前面一个例子中事务的功能。

```
LOCKTABLEinventoryWRITESELECTQuantityFROMinventoryWHEREItem='book';
...
UPDATEinventorySETQuantity=11WHEREItem='book';UNLOCKTABLES
```

这里，我们用一个select语句取出初始数据，通过一些计算，用update语句将新值更新到表中。包含有WRITE关键字的LOCKTABLE语句可以保证在UNLOCKTABLES命令被执行之前，不会有其它的访问来对inventory进行插入、更新或者删除的操作。

6、使用外键

锁定表的方法可以维护数据的完整性，但是它却不能保证数据的关联性。这个时候我们就可以使用外键。

例如，外键可以保证每一条销售记录都指向某一个存在的客户。在这里，外键可以把customerinfo表中的CustomerID映射到salesinfo表中CustomerID，任何一条没有合法CustomerID的记录都不会被更新或插入到salesinfo中。

```
 
```

1. `CREATETABLEcustomerinfo( CustomerIDINTNOTNULL,PRIMARYKEY(CustomerID))TYPE=INNODB;`
2.  
3. `CREATETABLEsalesinfo( SalesIDINTNOTNULL,CustomerIDINTNOTNULL,`
4.  
5. `PRIMARYKEY(CustomerID,SalesID),`
6.  
7. `FOREIGNKEY(CustomerID)REFERENCEScustomerinfo(CustomerID)ONDELETECASCADE)TYPE=INNODB;`

注意例子中的参数“ONDELETECASCADE”。该参数保证当customerinfo表中的一条客户记录被删除的时候，salesinfo表中所有与该客户相关的记录也会被自动删除。如果要在MySQL中使用外键，一定要记住在创建表的时候将表的类型定义为事务安全表InnoDB类型。该类型不是MySQL表的默认类型。定义的方法是在CREATETABLE语句中加上TYPE=INNODB。如例中所示。

7、使用索引

索引是提高数据库性能的常用方法，它可以令数据库服务器以比没有索引快得多的速度检索特定的行，尤其是在查询语句当中包含有MAX(),MIN()和ORDERBY这些命令的时候，性能提高更为明显。

那该对哪些字段建立索引呢？

一般说来，索引应建立在那些将用于JOIN,WHERE判断和ORDERBY排序的字段上。尽量不要对数据库中某个含有大量重复的值的字段建立索引。对于一个ENUM类型的字段来说，出现大量重复值是很有可能的情况

例如customerinfo中的“province”..字段，在这样的字段上建立索引将不会有什么帮助；相反，还有可能降低数据库的性能。我们在创建表的时候可以同时创建合适的索引，也可以使用ALTERTABLE或CREATEINDEX在以后创建索引。此外，MySQL从版本3.23.23开始支持全文索引和搜索。全文索引在MySQL中是一个FULLTEXT类型索引，但仅能用于MyISAM类型的表。对于一个大的数据库，将数据装载到一个没有FULLTEXT索引的表中，然后再使用ALTERTABLE或CREATEINDEX创建索引，将是非常快的。但如果将数据装载到一个已经有FULLTEXT索引的表中，执行过程将会非常慢。

8、优化的查询语句

绝大多数情况下，使用索引可以提高查询的速度，但如果SQL语句使用不恰当的话，索引将无法发挥它应有的作用。

下面是应该注意的几个方面。

- 首先，最好是在相同类型的字段间进行比较的操作。

  > 在MySQL3.23版之前，这甚至是一个必须的条件。例如不能将一个建有索引的INT字段和BIGINT字段进行比较；但是作为特殊的情况，在CHAR类型的字段和VARCHAR类型字段的字段大小相同的时候，可以将它们进行比较。

- 其次，在建有索引的字段上尽量不要使用函数进行操作。

> 例如，在一个DATE类型的字段上使用YEAE()函数时，将会使索引不能发挥应有的作用。所以，下面的两个查询虽然返回的结果一样，但后者要比前者快得多。

- 第三，在搜索字符型字段时，我们有时会使用LIKE关键字和通配符，这种做法虽然简单，但却也是以牺牲系统性能为代价的。

例如下面的查询将会比较表中的每一条记录。

```
SELECT*FROMbooks

WHEREnamelike"MySQL%"
```

但是如果换用下面的查询，返回的结果一样，但速度就要快上很多：

```
SELECT*FROMbooks

WHERE name＞="MySQL"and name＜"MySQM"
```

最后，应该注意避免在查询中让MySQL进行自动类型转换，因为转换过程也会使索引变得不起作用。

 

#### 数据库索引

**一、数据索引是干什么用的呢？**

数据库索引其实就是为了使查询数据效率快。

**二、数据库索引有哪些呢？**

1. 聚集索引（主键索引）：在数据库里面，所有行数都会按照主键索引进行排序。
2. 非聚集索引：就是给普通字段加上索引。
3. 联合索引：就是好几个字段组成的索引，称为联合索引。

```
key ``'idx_age_name_sex'` `(``'age'``,``'name'``,``'sex'``)
```

 联合索引遵从最左前缀原则，什么意思呢，就比如说一张学生表里面的联合索引如上面所示，那么下面A,B,C,D,E,F哪个会走索引呢？

```
A:select * from student where age = ``16` `and name = ``'小张'``
B:select * from student where name = ``'小张'` `and sex = ``'男'``
C:select * from student where name = ``'小张'` `and sex = ``'男'` `and age = ``18``
D:select * from student where age > ``20` `and name = ``'小张'``<br>
E:select * from student where age != ``15` `and name = ``'小张'``<br>F:select * from student where age = ``15` `and name != ``'小张'
```

 A遵从最左匹配原则，age是在最左边，所以A走索引；

 B直接从name开始，没有遵从最左匹配原则，所以不走索引；

 C虽然从name开始，但是有索引最左边的age，mysql内部会自动转成where age = '18' and name = '小张'  and sex = '男' 这种，所以还是遵从最左匹配原则；

 D这个是因为age>20是范围，范围字段会结束索引对范围后面索引字段的使用，所以只有走了age这个索引；

 E这个虽然遵循最左匹配原则，但是不走索引，因为!= 不走索引；

 F这个只走age索引，不走name索引，原因如上；

**三、有哪些列子不走索引呢？**

 表student中两个字段age,name加了索引

```
key ``'idx_age'` `(``'age'``),``key ``'idx_name'` `(``'name'``)
```

  1.Like这种就是%在前面的不走索引，在后面的走索引

```
A:select * from student where ``'name'` `like ``'王%'``B:select * from student where ``'name'` `like ``'%小'
```

 A走索引，B不走索引

 2.用索引列进行计算的，不走索引

```
A:select * from student where age = ``10``+``8``B:select * from student where age + ``8` `= ``18
```

 A走索引，B不走索引

 3.对索引列用函数了，不走索引

```
A:select * from student where concat(``'name'``,``'哈'``) =``'王哈哈'``;``B:select * from student where name = concat(``'王哈'``,``'哈'``);
```

 A不走索引，B走索引

 \4. 索引列用了!= 不走索引,如下：

```
select * from student where age != ``18
```

 **四、为什么索引用B+树？**

 这个可以参考[什么是B+树](https://www.cnblogs.com/wwxzdl/p/11089358.html)

**五、索引在磁盘上的存储？**

 聚集索引和非聚集索引存储的不相同，那么来说下都是怎么存储的？

 有一张学生表

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

create table `student` (
`id` int(11) not null auto_increment comment '主键id',
`name` varchar(50) not null default '' comment '学生姓名',
`age` int(11) not null default 0 comment '学生年龄',
primary key (`id`),
key `idx_age` (`age`),
key `idx_name` (`name`)
) ENGINE=InnoDB default charset=utf8 comment ='学生信息';

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

 表中内容如下

  ![img](https://img2018.cnblogs.com/blog/1724040/201907/1724040-20190702130722050-223991320.png)

id 为主键索引，name和age为非聚集索引

1.聚集索引在磁盘中的存储

<img src="https://img2018.cnblogs.com/blog/1724040/201907/1724040-20190702132817870-805725869.png" alt="img" style="zoom:80%;" />

 聚集索引叶子结点存储是表里面的所有行数据；

 每个数据页在不同的磁盘上面；

如果要查找id=5的数据，那么先把磁盘0读入内存，然后用二分法查找id=5的数在3和6之间，然后通过指针p1查找到磁盘2的地址，然后将磁盘2读入内存中，用二分查找方式查找到id=5的数据。

2.非聚集索引在磁盘中的存储

<img src="https://img2018.cnblogs.com/blog/1724040/201907/1724040-20190702133841357-2122435579.png" alt="img" style="zoom: 80%;" />

叶子结点存储的是聚集索引键，而不存储表里面所有的行数据，所以在查找的时候，只能查找到聚集索引键，再通过聚集索引去表里面查找到数据。

如果要查找到name = 小徐，首先将磁盘0加载到内存中，然后用二分查找的方法查到在指针p1所指的地址上，然后通过指针p1所指的地址可知道在磁盘2上面，然后通过二分查找法得知小徐id=4；

然后在根据id=4将磁盘0加载到内存中，然后通过二分查找的方法查到在指针p1所指的地址上，然后通过指针p1所指的地址可知道在磁盘2上面，然后通过id=4查找出郑正行数据，就查找出name=小徐的数据了。

### 优化Mysql数据库的8个方法

本文通过8个方法优化Mysql数据库：创建索引、复合索引、索引不会包含有NULL值的列、使用短索引、排序的索引问题、like语句操作、不要在列上进行运算、不使用NOT IN和<>操作

**1、创建索引**
对于查询占主要的应用来说，索引显得尤为重要。很多时候性能问题很简单的就是因为我们忘了添加索引而造成的，或者说没有添加更为有效的索引导致。如果不加索引的话，那么查找任何哪怕只是一条特定的数据都会进行一次全表扫描，如果一张表的数据量很大而符合条件的结果又很少，那么不加索引会引起致命的性能下降。但是也不是什么情况都非得建索引不可，比如性别可能就只有两个值，建索引不仅没什么优势，还会影响到更新速度，这被称为过度索引。
**2、复合索引**
比如有一条语句是这样的：select * from users where area='beijing' and age=22;
如果我们是在area和age上分别创建单个索引的话，由于mysql查询每次只能使用一个索引，所以虽然这样已经相对不做索引时全表扫描提高了很多效率，但是如果在area、age两列上创建复合索引的话将带来更高的效率。如果我们创建了(area, age, salary)的复合索引，那么其实相当于创建了(area,age,salary)、(area,age)、(area)三个索引，这被称为最佳左前缀特性。因此我们在创建复合索引时应该将最常用作限制条件的列放在最左边，依次递减。
**3、索引不会包含有NULL值的列**
只要列中包含有NULL值都将不会被包含在索引中，复合索引中只要有一列含有NULL值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为NULL。
**4、使用短索引**
对串列进行索引，如果可能应该指定一个前缀长度。例如，如果有一个CHAR(255)的 列，如果在前10 个或20 个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。
**5、排序的索引问题**
mysql查询只使用一个索引，因此如果where子句中已经使用了索引的话，那么order by中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引。
**6、like语句操作**
一般情况下不鼓励使用like操作，如果非使用不可，如何使用也是一个问题。like “%aaa%” 不会使用索引而like “aaa%”可以使用索引。
**7、不要在列上进行运算**
select * from users where YEAR(adddate)<2007;
将在每个行上进行运算，这将导致索引失效而进行全表扫描，因此我们可以改成
select * from users where adddate<‘2007-01-01';
**8、不使用NOT IN和<>操作**
NOT IN和<>操作都不会使用索引将进行全表扫描。NOT IN可以NOT EXISTS代替，id<>3则可使用id>3 or id<3来代替。

 

 

 

 

#### 百万级数据库优化方案

 

网上关于SQL优化的教程很多，但是比较杂乱。近日有空整理了一下，写出来跟大家分享一下，其中有错误和不足的地方，还请大家纠正补充。

这篇文章我花费了大量的时间查找资料、修改、排版，希望大家阅读之后，感觉好的话推荐给更多的人，让更多的人看到、纠正以及补充。

 

1.对查询进行优化，要尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。


2.应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：

```
select id from t where num is null
```

最好不要给数据库留NULL，尽可能的使用 NOT NULL填充数据库.

备注、描述、评论之类的可以设置为 NULL，其他的，最好不要使用NULL。

不要以为 NULL 不需要空间，比如：char(100) 型，在字段建立时，空间就固定了， 不管是否插入值（NULL也包含在内），都是占用 100个字符的空间的，如果是varchar这样的变长字段， null 不占用空间。


可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：

```
select id from t where num = 0
```


3.应尽量避免在 where 子句中使用 != 或 <> 操作符，否则将引擎放弃使用索引而进行全表扫描。

4.应尽量避免在 where 子句中使用 or 来连接条件，如果一个字段有索引，一个字段没有索引，将导致引擎放弃使用索引而进行全表扫描，如：

```
select id from t where num=10 or Name = 'admin'
```

可以这样查询：

```
select id from t where num = 10
union all
select id from t where Name = 'admin'
```


5.in 和 not in 也要慎用，否则会导致全表扫描，如：

```
select id from t where num in(1,2,3)
```

对于连续的数值，能用 between 就不要用 in 了：

```
select id from t where num between 1 and 3
```

很多时候用 exists 代替 in 是一个好的选择：

```
select num from a where num in(select num from b)
```

用下面的语句替换：

```
select num from a where exists(select 1 from b where num=a.num)
```

 

6.下面的查询也将导致全表扫描：

```
select id from t where name like ‘%abc%’
```

若要提高效率，可以考虑全文检索。

7.如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然 而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描：

```
select id from t where num = @num
```

可以改为强制查询使用索引：

```
select id from t with(index(索引名)) where num = @num
```

.应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如：

```
select id from t where num/2 = 100
```

应改为:

```
select id from t where num = 100*2
```


9.应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如：

```
select id from t where substring(name,1,3) = ’abc’       -–name以abc开头的id
select id from t where datediff(day,createdate,’2005-11-30′) = 0    -–‘2005-11-30’    --生成的id
```

应改为:

```
select id from t where name like 'abc%'
select id from t where createdate >= '2005-11-30' and createdate < '2005-12-1'
```


10.不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。

11.在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。

12.不要写一些没有意义的查询，如需要生成一个空表结构：

```
select col1,col2 into #t from t where 1=0
```

这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样：
create table #t(…)

13.Update 语句，如果只更改1、2个字段，不要Update全部字段，否则频繁调用会引起明显的性能消耗，同时带来大量日志。

14.对于多张大数据量（这里几百条就算大了）的表JOIN，要先分页再JOIN，否则逻辑读会很高，性能很差。

15.select count(*) from table；这样不带任何条件的count会引起全表扫描，并且没有任何业务意义，是一定要杜绝的。


16.索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有 必要。

17.应尽可能的避免更新 clustered 索引数据列，因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。

18.尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连 接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。

19.尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。

20.任何地方都不要使用 select * from t ，用具体的字段列表代替“*”，不要返回用不到的任何字段。

21.尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。

\22. 避免频繁创建和删除临时表，以减少系统表资源的消耗。临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件， 最好使用导出表。

23.在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。

24.如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。

25.尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。

26.使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。

27.与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时 间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。

28.在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF 。无需在执行存储过程和触发器的每个语句后向客户端发送 DONE_IN_PROC 消息。

29.尽量避免大事务操作，提高系统并发能力。

30.尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。

 

实际案例分析：拆分大的 DELETE 或INSERT 语句，批量提交SQL语句
　　如果你需要在一个在线的网站上去执行一个大的 DELETE 或 INSERT 查询，你需要非常小心，要避免你的操作让你的整个网站停止相应。因为这两个操作是会锁表的，表一锁住了，别的操作都进不来了。
　　Apache 会有很多的子进程或线程。所以，其工作起来相当有效率，而我们的服务器也不希望有太多的子进程，线程和数据库链接，这是极大的占服务器资源的事情，尤其是内存。
　　如果你把你的表锁上一段时间，比如30秒钟，那么对于一个有很高访问量的站点来说，这30秒所积累的访问进程/线程，数据库链接，打开的文件数，可能不仅仅会让你的WEB服务崩溃，还可能会让你的整台服务器马上挂了。
　　所以，如果你有一个大的处理，你一定把其拆分，使用 LIMIT oracle(rownum),sqlserver(top)条件是一个好的方法。下面是一个mysql示例：

```
while(1){

 　　//每次只做1000条

　　 mysql_query(“delete from logs where log_date <= ’2012-11-01’ limit 1000”);

 　　if(mysql_affected_rows() == 0){

　　 　　//删除完成，退出！
　　 　　break；
　　}

//每次暂停一段时间，释放表让其他进程/线程访问。
usleep(50000)

}
```

 

好了，到这里就写完了。我知道还有很多没有写到的，还请大家补充。后面有空会介绍一些SQL优化工具给大家。让我们一起学习，一起进步吧！

 

 

#### 运维角度浅谈MySQL数据库优化

 一个成熟的数据库架构并不是一开始设计就具备高可用、高伸缩等特性的，它是随着用户量的增加，基础架构才逐渐完善。这篇博文主要谈MySQL数据库发展周期中所面临的问题及优化方案，暂且抛开前端应用不说，大致分为以下五个阶段：

**1、数据库表设计**

 项目立项后，开发部根据产品部需求开发项目，开发工程师工作其中一部分就是对表结构设计。对于数据库来说，这点很重要，如果设计不当，会直接影响访问速度和用户体验。影响的因素很多，比如慢查询、低效的查询语句、没有适当建立索引、数据库堵塞（死锁）等。当然，有测试工程师的团队，会做压力测试，找bug。对于没有测试工程师的团队来说，大多数开发工程师初期不会太多考虑数据库设计是否合理，而是尽快完成功能实现和交付，等项目有一定访问量后，隐藏的问题就会暴露，这时再去修改就不是这么容易的事了。

**2、数据库部署**

 该运维工程师出场了，项目初期访问量不会很大，所以单台部署足以应对在1500左右的QPS（每秒查询率）。考虑到高可用性，可采用MySQL主从复制+Keepalived做双击热备，常见集群软件有Keepalived、Heartbeat。

双机热备博文：http://lizhenliang.blog.51cto.com/7876557/1362313

**3、数据库性能优化**

 如果将MySQL部署到普通的X86服务器上，在不经过任何优化情况下，MySQL理论值正常可以处理2000左右QPS，经过优化后，有可能会提升到2500左右QPS，否则，访问量当达到1500左右并发连接时，数据库处理性能就会变慢，而且硬件资源还很富裕，这时就该考虑软件问题了。那么怎样让数据库最大化发挥性能呢？一方面可以单台运行多个MySQL实例让服务器性能发挥到最大化，另一方面是对数据库进行优化，往往操作系统和数据库默认配置都比较保守，会对数据库发挥有一定限制，可对这些配置进行适当的调整，尽可能的处理更多连接数。

具体优化有以下三个层面：

 **3.1 数据库配置优化**

 MySQL常用有两种存储引擎，一个是MyISAM，不支持事务处理，读性能处理快，表级别锁。另一个是InnoDB，支持事务处理（ACID），设计目标是为处理大容量数据发挥最大化性能，行级别锁。

 表锁：开销小，锁定粒度大，发生死锁概率高，相对并发也低。

 行锁：开销大，锁定粒度小，发生死锁概率低，相对并发也高。

 为什么会出现表锁和行锁呢？主要是为了保证数据的完整性，举个例子，一个用户在操作一张表，其他用户也想操作这张表，那么就要等第一个用户操作完，其他用户才能操作，表锁和行锁就是这个作用。否则多个用户同时操作一张表，肯定会数据产生冲突或者异常。

 根据以上看来，使用InnoDB存储引擎是最好的选择，也是MySQL5.5以后版本中默认存储引擎。每个存储引擎相关联参数比较多，以下列出主要影响数据库性能的参数。

 公共参数默认值：

| 123456 | `max_connections = 151``#同时处理最大连接数，推荐设置最大连接数是上限连接数的80%左右  ``sort_buffer_size = 2M``#查询排序时缓冲区大小，只对order by和group by起作用，可增大此值为16M``open_files_limit = 1024 ``#打开文件数限制，如果show global status like 'open_files'查看的值等于或者大于open_files_limit值时，程序会无法连接数据库或卡死` |
| ------ | ------------------------------------------------------------ |
|        |                                                              |

 MyISAM参数默认值：

| 12345678910 | `key_buffer_size = 16M``#索引缓存区大小，一般设置物理内存的30-40%``read_buffer_size = 128K ``#读操作缓冲区大小，推荐设置16M或32M``query_cache_type = ON``#打开查询缓存功能``query_cache_limit = 1M ``#查询缓存限制，只有1M以下查询结果才会被缓存，以免结果数据较大把缓存池覆盖``query_cache_size = 16M ``#查看缓冲区大小，用于缓存SELECT查询结果，下一次有同样SELECT查询将直接从缓存池返回结果，可适当成倍增加此值` |
| ----------- | ------------------------------------------------------------ |
|             |                                                              |

 InnoDB参数默认值：

| 12345678910 | `innodb_buffer_pool_size = 128M``#索引和数据缓冲区大小，一般设置物理内存的60%-70%``innodb_buffer_pool_instances = 1  ``#缓冲池实例个数，推荐设置4个或8个``innodb_flush_log_at_trx_commit = 1 ``#关键参数，0代表大约每秒写入到日志并同步到磁盘，数据库故障会丢失1秒左右事务数据。1为每执行一条SQL后写入到日志并同步到磁盘，I/O开销大，执行完SQL要等待日志读写，效率低。2代表只把日志写入到系统缓存区，再每秒同步到磁盘，效率很高，如果服务器故障，才会丢失事务数据。对数据安全性要求不是很高的推荐设置2，性能高，修改后效果明显。``innodb_file_per_table = OFF ``#默认是共享表空间，共享表空间idbdata文件不断增大，影响一定的I/O性能。推荐开启独立表空间模式，每个表的索引和数据都存在自己独立的表空间中，可以实现单表在不同数据库中移动。``innodb_log_buffer_size = 8M ``#日志缓冲区大小，由于日志最长每秒钟刷新一次，所以一般不用超过16M` |
| ----------- | ------------------------------------------------------------ |
|             |                                                              |

 **3.2 系统内核优化**

 大多数MySQL都部署在linux系统上，所以操作系统的一些参数也会影响到MySQL性能，以下对linux内核进行适当优化。

| 12345678910 | `net.ipv4.tcp_fin_timeout = 30``#TIME_WAIT超时时间，默认是60s``net.ipv4.tcp_tw_reuse = 1  ``#1表示开启复用，允许TIME_WAIT socket重新用于新的TCP连接，0表示关闭``net.ipv4.tcp_tw_recycle = 1 ``#1表示开启TIME_WAIT socket快速回收，0表示关闭``net.ipv4.tcp_max_tw_buckets = 4096  ``#系统保持TIME_WAIT socket最大数量，如果超出这个数，系统将随机清除一些TIME_WAIT并打印警告信息``net.ipv4.tcp_max_syn_backlog = 4096``#进入SYN队列最大长度，加大队列长度可容纳更多的等待连接` |
| ----------- | ------------------------------------------------------------ |
|             |                                                              |

 在linux系统中，如果进程打开的文件句柄数量超过系统默认值1024，就会提示“too many files open”信息，所以要调整打开文件句柄限制。

| 1234 | `# vi /etc/security/limits.conf #加入以下配置，*代表所有用户，也可以指定用户，重启系统生效``* soft nofile 65535``* hard nofile 65535``# ulimit -SHn 65535  #立刻生效` |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

 **3.3 硬件配置**

 加大物理内存，提高文件系统性能。linux内核会从内存中分配出缓存区（系统缓存和数据缓存）来存放热数据，通过文件系统延迟写入机制，等满足条件时（如缓存区大小到达一定百分比或者执行sync命令）才会同步到磁盘。也就是说物理内存越大，分配缓存区越大，缓存数据越多。当然，服务器故障会丢失一定的缓存数据。

 SSD硬盘代替SAS硬盘，将RAID级别调整为RAID1+0，相对于RAID1和RAID5有更好的读写性能（IOPS），毕竟数据库的压力主要来自磁盘I/O方面。

**4、数据库架构扩展**

 随着业务量越来越大，单台数据库服务器性能已无法满足业务需求，该考虑加机器了，该做集群了~~~。主要思想是分解单台数据库负载，突破磁盘I/O性能，热数据存放缓存中，降低磁盘I/O访问频率。

 **4.1 主从复制与读写分离**

 因为生产环境中，数据库大多都是读操作，所以部署一主多从架构，主数据库负责写操作，并做双击热备，多台从数据库做负载均衡，负责读操作，主流的负载均衡器有LVS、HAProxy、Nginx。

 怎么来实现读写分离呢？大多数企业是在代码层面实现读写分离，效率比较高。另一个种方式通过代理程序实现读写分离，企业中应用较少，常见代理程序有MySQL Proxy、Amoeba。在这样数据库集群架构中，大大增加数据库高并发能力，解决单台性能瓶颈问题。如果从数据库一台从库能处理2000 QPS，那么5台就能处理1w QPS，数据库横向扩展性也很容易。

 有时，面对大量写操作的应用时，单台写性能达不到业务需求。如果做双主，就会遇到数据库数据不一致现象，产生这个原因是在应用程序不同的用户会有可能操作两台数据库，同时的更新操作造成两台数据库数据库数据发生冲突或者不一致。在单库时MySQL利用存储引擎机制表锁和行锁来保证数据完整性，怎样在多台主库时解决这个问题呢？有一套基于perl语言开发的主从复制管理工具，叫MySQL-MMM（Master-Master replication managerfor Mysql，Mysql主主复制管理器），这个工具最大的优点是在同一时间只提供一台数据库写操作，有效保证数据一致性。

 主从复制博文：http://lizhenliang.blog.51cto.com/7876557/1290431

 读写分离博文：http://lizhenliang.blog.51cto.com/7876557/1305083

 MySQL-MMM博文：http://lizhenliang.blog.51cto.com/7876557/1354576

 **4.2 增加缓存**

 给数据库增加缓存系统，把热数据缓存到内存中，如果缓存中有要请求的数据就不再去数据库中返回结果，提高读性能。缓存实现有本地缓存和分布式缓存，本地缓存是将数据缓存到本地服务器内存中或者文件中。分布式缓存可以缓存海量数据，扩展性好，主流的分布式缓存系统有memcached、redis，memcached性能稳定，数据缓存在内存中，速度很快，QPS可达8w左右。如果想数据持久化就选择用redis，性能不低于memcached。

 工作过程：

 [![wKiom1VukrqyM-JcAABPhCy-LOM409.jpg](http://s3.51cto.com/wyfs02/M02/6D/E6/wKiom1VukrqyM-JcAABPhCy-LOM409.jpg)](http://s3.51cto.com/wyfs02/M02/6D/E6/wKiom1VukrqyM-JcAABPhCy-LOM409.jpg)

 **4.3 分库**

 分库是根据业务不同把相关的表切分到不同的数据库中，比如web、bbs、blog等库。如果业务量很大，还可将切分后的库做主从架构，进一步避免单个库压力过大。

 **4.4 分表**

 数据量的日剧增加，数据库中某个表有几百万条数据，导致查询和插入耗时太长，怎么能解决单表压力呢？你就该考虑是否把这个表拆分成多个小表，来减轻单个表的压力，提高处理效率，此方式称为分表。

 分表技术比较麻烦，要修改程序代码里的SQL语句，还要手动去创建其他表，也可以用merge存储引擎实现分表，相对简单许多。分表后，程序是对一个总表进行操作，这个总表不存放数据，只有一些分表的关系，以及更新数据的方式，总表会根据不同的查询，将压力分到不同的小表上，因此提高并发能力和磁盘I/O性能。

 分表分为垂直拆分和水平拆分：

 垂直拆分：把原来的一个很多字段的表拆分多个表，解决表的宽度问题。你可以把不常用的字段单独放到一个表中，也可以把大字段独立放一个表中，或者把关联密切的字段放一个表中。

 水平拆分：把原来一个表拆分成多个表，每个表的结构都一样，解决单表数据量大的问题。

 **4.5 分区**

 分区就是把一张表的数据根据表结构中的字段（如range、list、hash等）分成多个区块，这些区块可以在一个磁盘上，也可以在不同的磁盘上，分区后，表面上还是一张表，但数据散列在多个位置，这样一来，多块硬盘同时处理不同的请求，从而提高磁盘I/O读写性能，实现比较简单。

注：增加缓存、分库、分表和分区主要由程序猿来实现。

**5、数据库维护**

 数据库维护是运维工程师或者DBA主要工作，包括性能监控、性能分析、性能调优、数据库备份和恢复等。

 **5.1 性能状态关键指标**

 QPS，Queries Per Second：每秒查询数，一台数据库每秒能够处理的查询次数

 TPS，Transactions Per Second：每秒处理事务数

 通过show status查看运行状态，会有300多条状态信息记录，其中有几个值帮可以我们计算出QPS和TPS，如下：

 Uptime：服务器已经运行的实际，单位秒

 Questions：已经发送给数据库查询数

 Com_select：查询次数，实际操作数据库的

 Com_insert：插入次数

 Com_delete：删除次数

 Com_update：更新次数

 Com_commit：事务次数

 Com_rollback：回滚次数

 那么，计算方法来了，基于Questions计算出QPS：

| 12   | ` ``mysql> show global status like ``'Questions'``;`` ``mysql> show global status like ``'Uptime'``;` |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

 QPS = Questions / Uptime

 基于Com_commit和Com_rollback计算出TPS：

| 123  | ` ``mysql> show global status like ``'Com_commit'``;`` ``mysql> show global status like ``'Com_rollback'``;`` ``mysql> show global status like ``'Uptime'``;` |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

 TPS = (Com_commit + Com_rollback) / Uptime

 另一计算方式：基于Com_select、Com_insert、Com_delete、Com_update计算出QPS

| 1    | ` ``mysql> show global status where Variable_name ``in``(``'com_select'``,``'com_insert'``,``'com_delete'``,``'com_update'``);` |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

 等待1秒再执行，获取间隔差值，第二次每个变量值减去第一次对应的变量值，就是QPS

 TPS计算方法：

| 1    | ` ``mysql> show global status where Variable_name ``in``(``'com_insert'``,``'com_delete'``,``'com_update'``);` |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

 计算TPS，就不算查询操作了，计算出插入、删除、更新四个值即可。

 经网友对这两个计算方式的测试得出，当数据库中myisam表比较多时，使用Questions计算比较准确。当数据库中innodb表比较多时，则以Com_*计算比较准确。

 **5.2 开启慢查询日志**

 MySQL开启慢查询日志，分析出哪条SQL语句比较慢，使用set设置变量，重启服务失效，可以在my.cnf添加参数永久生效。

| 1234 | `mysql> ``set` `global slow-query-log=on ``#开启慢查询功能``mysql> ``set` `global slow_query_log_file=``'/var/log/mysql/mysql-slow.log'``; ``#指定慢查询日志文件位置``mysql> ``set` `global log_queries_not_using_indexes=on;  ``#记录没有使用索引的查询``mysql> ``set` `global long_query_time=1;  ``#只记录处理时间1s以上的慢查询` |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

 分析慢查询日志，可以使用MySQL自带的mysqldumpslow工具，分析的日志较为简单。

 \# mysqldumpslow -t 3 /var/log/mysql/mysql-slow.log   #查看最慢的前三个查询

 也可以使用percona公司的pt-query-digest工具，日志分析功能全面，可分析slow log、binlog、general log。

 分析慢查询日志：pt-query-digest /var/log/mysql/mysql-slow.log

 分析binlog日志：mysqlbinlog mysql-bin.000001 >mysql-bin.000001.sql 

 pt-query-digest --type=binlog mysql-bin.000001.sql 

 分析普通日志：pt-query-digest --type=genlog localhost.log

 **5.3 数据库备份**

 备份数据库是最基本的工作，也是最重要的，否则后果很严重，你懂得！但由于数据库比较大，上百G，往往备份都很耗费时间，所以就该选择一个效率高的备份策略，对于数据量大的数据库，一般都采用增量备份。常用的备份工具有mysqldump、mysqlhotcopy、xtrabackup等，mysqldump比较适用于小的数据库，因为是逻辑备份，所以备份和恢复耗时都比较长。mysqlhotcopy和xtrabackup是物理备份，备份和恢复速度快，不影响数据库服务情况下进行热拷贝，建议使用xtrabackup，支持增量备份。

 Xtrabackup备份工具使用博文：http://lizhenliang.blog.51cto.com/7876557/1612800

 **5.4 数据库修复**

 有时候MySQL服务器突然断电、异常关闭，会导致表损坏，无法读取表数据。这时就可以用到MySQL自带的两个工具进行修复，myisamchk和mysqlcheck。

 myisamchk：只能修复myisam表，需要停止数据库

 常用参数：

 -f --force   强制修复，覆盖老的临时文件，一般不使用

 -r --recover  恢复模式

 -q --quik   快速恢复

 -a --analyze  分析表

 -o --safe-recover 老的恢复模式，如果-r无法修复，可以使用此参数试试

 -F --fast   只检查没有正常关闭的表

 快速修复weibo数据库:

 \# cd /var/lib/mysql/weibo 

 \# myisamchk -r -q *.MYI

 mysqlcheck：myisam和innodb表都可以用，不需要停止数据库，如修复单个表，可在数据库后面添加表名，以空格分割

 常用参数：

 -a  --all-databases  检查所有的库

 -r  --repair  修复表

 -c  --check   检查表，默认选项

 -a  --analyze  分析表

 -o  --optimize 优化表

 -q  --quik  最快检查或修复表

 -F  --fast  只检查没有正常关闭的表

 快速修复weibo数据库:

 mysqlcheck -r -q -uroot -p123 weibo 

 5.5 另外，查看CPU和I/O性能方法

 \#查看CPU性能

![wKiom1VtPFmCEtY9AADbdiZbn9A400.jpg](http://s3.51cto.com/wyfs02/M02/6D/D9/wKiom1VtPFmCEtY9AADbdiZbn9A400.jpg)

 \#参数-P是显示CPU数，ALL为所有，也可以只显示第几颗CPU![wKioL1VtPpayB7WeAALQHX41buc367.jpg](http://s3.51cto.com/wyfs02/M00/6D/D5/wKioL1VtPpayB7WeAALQHX41buc367.jpg)

 \#查看I/O性能

[![wKiom1VtPSXTsI4zAAMkfVf2r-I743.jpg](http://s3.51cto.com/wyfs02/M00/6D/D9/wKiom1VtPSXTsI4zAAMkfVf2r-I743.jpg)](http://s3.51cto.com/wyfs02/M00/6D/D9/wKiom1VtPSXTsI4zAAMkfVf2r-I743.jpg)

 

 \#参数-m是以M单位显示，默认K

 \#%util：当达到100%时，说明I/O很忙。

 \#await：请求在队列中等待时间，直接影响read时间。

 I/O极限：IOPS（r/s+w/s）,一般RAID0/10在1200左右。（IOPS，每秒进行读写（I/O）操作次数）

 I/O带宽：在顺序读写模式下SAS硬盘理论值在300M/s左右，SSD硬盘理论值在600M/s左右。